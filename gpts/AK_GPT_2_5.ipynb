{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFLlaLi8FiQu1DolzN5sQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/gpts/AK_GPT_2_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_IrdEZGaS2R"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
        "sd_hf = model_hf.state_dict()\n",
        "\n",
        "for k,v in sd_hf.items():\n",
        "  print(k, v.shape)"
      ],
      "metadata": {
        "id": "_hVVayywad4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sd_hf[\"transformer.wpe.weight\"].view(-1)[:20]"
      ],
      "metadata": {
        "id": "2mZvMml2atLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"grey\")"
      ],
      "metadata": {
        "id": "evhFcQom4ffx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 150])\n",
        "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])\n",
        "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 200])"
      ],
      "metadata": {
        "id": "IyFzEeFb_Cfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(sd_hf[\"transformer.h.1.attn.c_attn.weight\"][:300, :300], cmap=\"gray\")"
      ],
      "metadata": {
        "id": "fU8HzCPz_wXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "set_seed(42)\n",
        "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "hZ0Cr6UYAff4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################\n",
        "#       GPT-MODEL-IMPLEMENTATION       #\n",
        "########################################"
      ],
      "metadata": {
        "id": "WOcOr9EzDE84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import inspect\n",
        "import os\n",
        "import time\n",
        "import torch.distributed as dist\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    # key, query, value projections for all heads, but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "    # output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGTP_SCALE_INIT = 1\n",
        "    # regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embed = config.n_embd\n",
        "    # not really a 'bias', more of a mask, but following the OpenAI/HF naming though\n",
        "    self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1, config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size() # batch size, sequence length, embedding dim (n_embed)\n",
        "    # calculate query, key, values for all heads in batch and move head forward to be the batch\n",
        "    # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * ns\n",
        "    # e.g. in GPT-2 (124M), n_heads=12, hs=64, so nh * hs = 768 channels in the transformer\n",
        "    qkv = self.c_attn(x)\n",
        "    q, k, v = qkv.split(self.n_embed, dim=2)\n",
        "    k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
        "    q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
        "    v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2) # (B, nh, T, hs)\n",
        "    # attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "\n",
        "    #att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "    #att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "    #att = F.softmax(att, dim=-1)\n",
        "    #y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "    y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "    # output projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "class TanhGELU(nn.Module):\n",
        "\n",
        "  def forward(self, input):\n",
        "    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 # max sequence length\n",
        "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|>\n",
        "  n_layer: int = 12 # number of layers\n",
        "  n_head: int = 12 # number of heads\n",
        "  n_embd: int = 768 # embedding dimension\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.transformer = nn.ModuleDict(dict(\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "        ln_f = nn.LayerNorm(config.n_embd)\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    # weight sharing scheme\n",
        "    self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "    # init params\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "        std *= (2 * self.config.n_layer)**-0.5\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    # idx is of shape (B,T)\n",
        "    B, T = idx.size()\n",
        "    assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "    # forward the token and position embeddings\n",
        "    pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "    pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "    tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "    x = tok_emb + pos_emb\n",
        "    # forward the blocks of the transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "     # forward the final layernorm and the classifier\n",
        "    x = self.transformer.ln_f(x)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "    loss = None\n",
        "\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type, override_args=None):\n",
        "      assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "      override_args = override_args or {} # default to empty dict\n",
        "      # only dropout can be overridden see more notes below\n",
        "      assert all(k == 'dropout' for k in override_args)\n",
        "      from transformers import GPT2LMHeadModel\n",
        "      print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "      # n_layer, n_head and n_embd are determined from model_type\n",
        "      config_args = {\n",
        "          'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "          'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "          'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "          'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "      }[model_type]\n",
        "      print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "      config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "      config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "\n",
        "      config = GPTConfig(**config_args)\n",
        "      model = GPT(config)\n",
        "      sd = model.state_dict()\n",
        "      sd_keys = sd.keys()\n",
        "      sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "      # init a huggingface/transformers model\n",
        "      model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "      sd_hf = model_hf.state_dict()\n",
        "\n",
        "      # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "      sd_keys_hf = sd_hf.keys()\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "      sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "      transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "      # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "      # this means that we have to transpose these weights when we import them\n",
        "      assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "      for k in sd_keys_hf:\n",
        "          if any(k.endswith(w) for w in transposed):\n",
        "              # special treatment for the Conv1D weights we need to transpose\n",
        "              assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k].t())\n",
        "          else:\n",
        "              # vanilla copy over the other parameters\n",
        "              assert sd_hf[k].shape == sd[k].shape\n",
        "              with torch.no_grad():\n",
        "                  sd[k].copy_(sd_hf[k])\n",
        "\n",
        "      return model\n",
        "\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "    # start with all of the candidate parameters (that require grad)\n",
        "    param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "    # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no\n",
        "    # i.e all weigh tensors in matmuls + embeddings decay, all biases and layernorms don't\n",
        "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "    nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "    optim_groups = [\n",
        "        {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
        "        {\"params\": nodecay_params, \"weight_decay\": 0.0}\n",
        "    ]\n",
        "    num_decay_params = sum(p.numel() for p in decay_params)\n",
        "    num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "    print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "    # Create AdamW optimizer and use the fused version if it is available\n",
        "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "    use_fused = fused_available and \"cuda\" in device\n",
        "    print(f\"using fused AdamW: {use_fused}\")\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-08)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "UdcFa35zEOde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderLite:\n",
        "\n",
        "  def __init__(self, B, T, process_rank, num_processes):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    self.process_rank = process_rank\n",
        "    self.num_processes = num_processes\n",
        "\n",
        "    # at init load tokens from disk and store them in memory\n",
        "    with open(\"input.txt\", \"r\") as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "\n",
        "    # state\n",
        "    self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
        "    x = (buf[:-1]).view(B, T) # inputs\n",
        "    y = (buf[1:]).view(B, T) # targets\n",
        "    # advance the position in the tensor\n",
        "    self.current_position += B * T * self.num_processes\n",
        "    # if loading the next batch would be out of bounds, reset\n",
        "    if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "      self.current_position = self.B * self.T * self.process_rank\n",
        "    return x,y\n",
        "\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# set up DDP (distributed data parallel)\n",
        "# torchrun command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "ddp = int(os.environ.get(\"RANK\", -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "  # use of DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "  assert torch.cuda.is_available(), \"for now i think we need CUDA for DDP\"\n",
        "  init_process_group(backend=\"nccl\")\n",
        "  ddp_rank = int(os.environ[\"RANK\"])\n",
        "  ddp_local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
        "  ddp_world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "  device = f\"cuda:{ddp_local_rank}\"\n",
        "  torch.cuda.set_device(device)\n",
        "  master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "else:\n",
        "  # vanilla, non-DDP run\n",
        "  ddp_rank = 0\n",
        "  ddp_local_rank = 0\n",
        "  ddp_world_size = 1\n",
        "  master_process = True\n",
        "  # attempt to autodetect the device\n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "  print(f\"using device: {device}\")\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(1337)\n",
        "\n",
        "total_batch_size = 524288 # 2**19, ~0.5M,  in number of tokens\n",
        "B = 4 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "  print(f\"total desired batch size: {total_batch_size}\")\n",
        "  print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "\n",
        "\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "#model = GPT.from_pretrained(\"gpt2\")\n",
        "model = GPT(GPTConfig())\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "WssqGVeUFJpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "# prefix tokens\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) # (5,8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "# generate! right now x is (B,T) where B = 5, T = 8\n",
        "# set the seed to 42\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "  # forward the model to get the logits\n",
        "  with torch.no_grad():\n",
        "    logits, _ = model(x) # (B, T, vocab_size)\n",
        "    # take the logits at the last position\n",
        "    logits = logits[:, -1, :] # (B, vocab_size)\n",
        "    # get the probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    # do top-k sampling of 50 (huggingface pipeline default)\n",
        "    # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "    # select a token from the top-k probabilities\n",
        "    ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "    # gather the corresponding indices\n",
        "    xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "    # append to the sequence\n",
        "    x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i, :max_length].tolist()\n",
        "  decoded = enc.decode(tokens)\n",
        "  print(\">\", decoded)"
      ],
      "metadata": {
        "id": "qoFt-bwKl9LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "############################\n",
        "# Tiny shakespeare dataset #\n",
        "############################\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open(\"input.txt\", \"r\") as f:\n",
        "  text = f.read()\n",
        "data = text[:1000] # first 1,000 characters\n",
        "print(data[:100])\n",
        "\n",
        "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size)\n",
        "\n",
        "# need the NVIDIA Ampere Architecture (T4 is Tesla Architecture and does not support it)\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# create model\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "if ddp:\n",
        "  model = DDP(model, device_ids=[ddp_local_rank])\n",
        "raw_model = model.module if ddp else model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "\n",
        "def get_lr(it):\n",
        "  # 1) linear warmup for warmup_iters steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it + 1) / warmup_steps\n",
        "  # 2) if it > lr_decay_iters, return min learning rate\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "  # 3) in between, use cosine decay down to min learning rate\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.5, 0.95), eps=1e-08)\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "  loss_accum = 0.0\n",
        "  for micro_step in range(grad_accum_steps):\n",
        "    x,y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "      logits, loss = model(x, y)\n",
        "    # we have to scale the loss to account for gradient accumulation,\n",
        "    # because the gradients just add on each successive backward().\n",
        "    # addition of gradients corresponds to a SUM in the objective, but\n",
        "    # instead of a SUM we want MEAN. Scale the loss here so it come out right\n",
        "    loss = loss / grad_accum_steps\n",
        "    loss_accum += loss.detach()\n",
        "    if ddp:\n",
        "      model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "    loss.backward()\n",
        "  if ddp:\n",
        "    dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  # determine and set the learning rate for this iteration\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1 = time.time()\n",
        "  dt = (t1 - t0) * 1000 # time difference in milliseconds\n",
        "  tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "  tokens_per_sec = tokens_processed / (t1 -t0)\n",
        "  if master_process:\n",
        "    print(f\"step {step}, loss: {loss_accum.item()}, lr: {lr:.4e}, norm: {norm:.4f}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec}\")\n",
        "\n",
        "if ddp:\n",
        "  destroy_process_group()"
      ],
      "metadata": {
        "id": "p0To0Awlrywz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "GyqWBngCRjr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "#     PLAYGROUND     #\n",
        "######################"
      ],
      "metadata": {
        "id": "zcJDbRUvTANd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "buf = torch.tensor(tokens[:24 + 1])\n",
        "x = buf[:-1].view(4, 6)\n",
        "y = buf[1:].view(4, 6)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "7cavYQ5HR0Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sd_hf[\"lm_head.weight\"] == sd_hf[\"transformer.wte.weight\"]).all()"
      ],
      "metadata": {
        "id": "HGbsPucPSbMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sd_hf[\"lm_head.weight\"].data_ptr())\n",
        "print(sd_hf[\"transformer.wte.weight\"].data_ptr())"
      ],
      "metadata": {
        "id": "1aEJAIuOsydd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standard deviation grows inside the residual stream\n",
        "x = torch.zeros(768)\n",
        "n = 100 # e.g. 100 layers\n",
        "\n",
        "for i in range(n):\n",
        "  x += n**-0.5 * torch.randn(768)\n",
        "\n",
        "print(x.std())"
      ],
      "metadata": {
        "id": "W-Hla2hJtB5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "# TOY EXAMPLE FOR GRADACCUMULATION #\n",
        "####################################\n",
        "\n",
        "import torch\n",
        "\n",
        "# super simple little MLP\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(16,32),\n",
        "    torch.nn.GELU(),\n",
        "    torch.nn.Linear(32,1)\n",
        ")\n",
        "\n",
        "torch.random.manual_seed(42)\n",
        "x = torch.randn(4, 16)\n",
        "y = torch.randn(4, 1)\n",
        "net.zero_grad()\n",
        "yhat = net(x)\n",
        "loss = torch.nn.functional.mse_loss(yhat, y)\n",
        "loss.backward()\n",
        "print(net[0].weight.grad.view(-1)[:10])\n",
        "\n",
        "# the loss objective here is (due to readuction='mean')\n",
        "# L = 1/4 * [\n",
        "#    (y[0] - yhat[0]) ** 2 +\n",
        "#    (y[1] - yhat[1]) ** 2 +\n",
        "#    (y[2] - yhat[2]) ** 2 +\n",
        "#    (y[3] - yhat[3]) ** 2\n",
        "#]\n",
        "# NOTE: 1/4!"
      ],
      "metadata": {
        "id": "T3L_v9fbyFXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now let's do it with grad_accum_steps of 4 and B = 1\n",
        "# the loss objective here is different because\n",
        "# accumulation in gradient <---> SUM in loss\n",
        "# i.e we instead get:\n",
        "# L0 = (y[0] - yhat[0]) ** 2\n",
        "# L1 = (y[1] - yhat[1]) ** 2\n",
        "# L2 = (y[2] - yhat[2]) ** 2\n",
        "# L3 = (y[3] - yhat[3]) ** 2\n",
        "# L = L0 + L1 + L2 + L3\n",
        "# NOTE: the \"normalizer\" of 1/4 is lost\n",
        "net.zero_grad()\n",
        "for i in range(4):\n",
        "  yhat = net(x[i])\n",
        "  loss = torch.nn.functional.mse_loss(yhat, y[i])\n",
        "  loss = loss / 4 # <-----this the normalizer\n",
        "  loss.backward()\n",
        "print(net[0].weight.grad.view(-1)[:10])"
      ],
      "metadata": {
        "id": "pulUWjKBrlD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Explanation of the main IMRPOVEMENT steps #\n",
        "#############################################"
      ],
      "metadata": {
        "id": "XbPh0Qfos_s5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.compile"
      ],
      "metadata": {
        "id": "VjS5aAucm2yA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sNRtin7sm7SD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}