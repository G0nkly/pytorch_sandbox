{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1qSTo1EPS3jDOzy7aY8zs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/gpts/DIY_AK_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "ij8tuG9igQYh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "# DATASET RETRIEVAL #\n",
        "#####################"
      ],
      "metadata": {
        "id": "vOE6izkkcXgR"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M-nuBUubq5B",
        "outputId": "67f03377-abd7-4f35-cb2f-482393107cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-21 10:55:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-09-21 10:55:50 (17.5 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"input.txt\", mode=\"r\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(text[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1nI2CsZcD2U",
        "outputId": "901f6dc9-cdbe-4d45-d6d7-f9658d1912cf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "# HYPERPARAMETERS #\n",
        "###################"
      ],
      "metadata": {
        "id": "pwiNVJYyiJFP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "batch_size = 32\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "embed_dim = 32\n",
        "n_epochs = 10\n",
        "learning_rate = 1e-04\n",
        "test_epochs = 200\n",
        "eval_iters = 100"
      ],
      "metadata": {
        "id": "g5J-q-fMiNtP"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################\n",
        "# TOKENIZATION AND DATALOADER #\n",
        "###############################"
      ],
      "metadata": {
        "id": "f7vKkDkbcNUu"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(sorted(set(text)))\n",
        "text_to_num = {v:k for k,v in enumerate(vocab)}\n",
        "num_to_text = {k:v for k,v in enumerate(vocab)}\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "E3yxlHxlcm3S"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda text: [text_to_num[t] for t in text]\n",
        "decode = lambda numbers: \"\".join([num_to_text[n] for n in numbers])"
      ],
      "metadata": {
        "id": "F6Aoo7z3dBYI"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decode(encode(\"Haubi\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MwWi_kfzeds3",
        "outputId": "bf12fe6e-923d-40ac-861d-35a1d9b5b2f4"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Haubi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text))\n",
        "train_idx = int(0.9 * len(data))\n",
        "train = data[:train_idx]\n",
        "test = data[train_idx:]\n",
        "len(train), len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_IMCrZOefWn",
        "outputId": "e452215c-7f89-4071-faa6-bae02e56fdf8"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1003854, 111540)"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  dataset = train if split == \"train\" else test\n",
        "  idx = torch.randint(0, len(train) - block_size, (batch_size,))\n",
        "  X = torch.stack([train[i:i +  block_size] for i in idx])\n",
        "  Y = torch.stack([train[i + 1: i + block_size + 1] for i in idx])\n",
        "  X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "o_6L3ABziYiV"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = get_batch(\"train\")\n",
        "X[1], Y[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqyYIA7wjQiv",
        "outputId": "d481ae98-e8ef-48e1-e446-05c36da0438b"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([57, 53, 53, 52,  1, 51, 39, 49]),\n",
              " tensor([53, 53, 52,  1, 51, 39, 49, 43]))"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(model):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [\"train\", \"test\"]:\n",
        "    test_loss = torch.zeros(test_epochs)\n",
        "    for epch in range(test_epochs):\n",
        "      Xtst_b, Ytst_b = get_batch(\"test\")\n",
        "      _, loss = model(Xtst_b, Ytst_b)\n",
        "\n",
        "      test_loss[epch] = loss.item()\n",
        "\n",
        "    out[split] = test_loss.mean(dim=-1)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  return out"
      ],
      "metadata": {
        "id": "OcG7SV-3PQd5"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "# BUILD MODELS #\n",
        "################"
      ],
      "metadata": {
        "id": "9vrslYjGjvkW"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleBiGramModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, targets=None):\n",
        "    logits = self.emb(x)\n",
        "    if targets is not None:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      return x, loss\n",
        "    return x, None\n",
        "\n",
        "  def generate(self, start_token, length):\n",
        "    \"\"\"\n",
        "    start_token: (B, 1) tensor with the initial token(s)\n",
        "    length: how many new tokens to generate\n",
        "    \"\"\"\n",
        "    for _ in range(length):\n",
        "      # only keep the last token\n",
        "      input = start_token[:, -block_size:]   # shape (B, 1)\n",
        "\n",
        "      # forward pass → logits for vocab\n",
        "      logits = self(input)          # (B, 1, C)\n",
        "\n",
        "      # take the logits at the last position\n",
        "      print(logits)\n",
        "      logits = logits[:, -1, :]     # (B, C)\n",
        "\n",
        "      # turn into probabilities\n",
        "      probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "\n",
        "      # sample next token for each batch\n",
        "      next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "\n",
        "      # append to sequence\n",
        "      start_token = torch.cat((start_token, next_token), dim=1)\n",
        "\n",
        "    return start_token\n"
      ],
      "metadata": {
        "id": "mc9p2yfTll7f"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleBiGramModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "AwGefE6kviC1"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1000):\n",
        "\n",
        "  if epoch % eval_iters == 0 or epoch == n_epochs - 1:\n",
        "    out = evaluate_model(model)\n",
        "    print(f\"Epoch: {epoch}, train loss: {out[\"train\"]:.4f}, test loss: {out[\"test\"]:.4f}\")\n",
        "\n",
        "  X_batch, Y_batch = get_batch(\"train\")\n",
        "  logits, loss = model(X_batch, Y_batch)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  X_test_batch, Y_test_batch = get_batch(\"test\")\n",
        "  _, test_loss = model(X_test_batch, Y_test_batch)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5OebzBWxinF",
        "outputId": "361ee325-c5aa-41fd-dc58-964694b8c56a"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, train loss: 4.5324, test loss: 4.5365\n",
            "Epoch: 9, train loss: 4.5455, test loss: 4.5298\n",
            "Epoch: 100, train loss: 4.5277, test loss: 4.5099\n",
            "Epoch: 200, train loss: 4.5083, test loss: 4.5124\n",
            "Epoch: 300, train loss: 4.5013, test loss: 4.5118\n",
            "Epoch: 400, train loss: 4.4914, test loss: 4.4942\n",
            "Epoch: 500, train loss: 4.4730, test loss: 4.4808\n",
            "Epoch: 600, train loss: 4.4668, test loss: 4.4583\n",
            "Epoch: 700, train loss: 4.4592, test loss: 4.4527\n",
            "Epoch: 800, train loss: 4.4480, test loss: 4.4429\n",
            "Epoch: 900, train loss: 4.4330, test loss: 4.4288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_output = model.generate(torch.randint(0, 65, (1, 8)), 30)\n",
        "output_list = generated_output.view(-1).tolist()\n",
        "\"\".join([decode(output_list)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "0l-VnHDCv8j-",
        "outputId": "32604610-6313-4292-a48e-0958e9903fa8"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[51, 57, 36, 61,  9,  8, 20, 24]]), None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "tuple indices must be integers or slices, not tuple",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2327694100.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerated_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2606145549.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, start_token, length)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# take the logits at the last position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m     \u001b[0;31m# (B, C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;31m# turn into probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMs_RLJIN2GA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}