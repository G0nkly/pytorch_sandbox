{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeeVd+ZsS+gGrXKEx763F6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/gpts/finetuning/TinyLlama_gsm8k_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hiDfRPc1kZ6p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "VnuU-lppujkf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "fELQRIhxu8iS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:200]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJLnFqWbwnn2",
        "outputId": "20ff485d-896d-4314-b496-6e9f91e7d87c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating train split: 100%|██████████████████████████████████████████████████████████| 7473/7473 [00:00<00:00, 646695.42 examples/s]\n",
            "Generating test split: 100%|███████████████████████████████████████████████████████████| 1319/1319 [00:00<00:00, 385874.80 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "  texts = [\n",
        "      f\"### Instruction:\\n{instruction}\\n### Response:\\n{out}\"\n",
        "      for instruction, out in zip(batch['question'], batch['answer'])\n",
        "  ]\n",
        "\n",
        "  tokens = tokenizer(\n",
        "      texts,\n",
        "      padding=\"max_length\",\n",
        "      max_length=256,\n",
        "      truncation=True,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "  tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "nF_z5Zk9wx3f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = data.map(tokenize, batched=True, remove_columns=data.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtDKgQccxlrG",
        "outputId": "218fdc03-265a-41ea-9728-d8f3a98a5dd0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|█████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 3031.41 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "  output_dir=\"./tinyllama-math-lora-tutorial\",\n",
        "  per_device_train_batch_size=8,\n",
        "  gradient_accumulation_steps=8,\n",
        "  learning_rate=1e-3,\n",
        "  num_train_epochs=50,\n",
        "  fp16=True,\n",
        "  logging_steps=20,\n",
        "  save_strategy=\"epoch\",\n",
        "  report_to=\"none\",\n",
        "  remove_unused_columns=False,\n",
        "  label_names=[\"labels\"]\n",
        ")"
      ],
      "metadata": {
        "id": "5DONRHVOyBZ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data,\n",
        "    processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "8T2az_goyvjp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "XUunvecczFCc",
        "outputId": "8c825f36-fa8d-4eeb-ab9a-3948245571b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 14:51, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.656800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.797200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.576700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.488900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.395900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.337300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.283300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.243700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.224400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=0.5669354009628296, metrics={'train_runtime': 896.7386, 'train_samples_per_second': 11.152, 'train_steps_per_second': 0.223, 'total_flos': 1.590741172224e+16, 'train_loss': 0.5669354009628296, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./tinyllama-lora-tuned-adapther-math\")\n",
        "tokenizer.save_pretrained(\"./tinyllama-lora-tuned-adapther-math\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItPvc00fzGth",
        "outputId": "a504d86a-e324-4a38-a3e6-aee5e88fe1a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tinyllama-lora-tuned-adapther-math/tokenizer_config.json',\n",
              " './tinyllama-lora-tuned-adapther-math/special_tokens_map.json',\n",
              " './tinyllama-lora-tuned-adapther-math/chat_template.jinja',\n",
              " './tinyllama-lora-tuned-adapther-math/tokenizer.model',\n",
              " './tinyllama-lora-tuned-adapther-math/added_tokens.json',\n",
              " './tinyllama-lora-tuned-adapther-math/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "APOclr-I4LwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}