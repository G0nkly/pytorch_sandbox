{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQdAegY6Y4kkRlpRLATLGY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vits/vlms/nanoVLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diXQ9DkumHGB"
      },
      "outputs": [],
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rbUm1X6rmtnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables"
      ],
      "metadata": {
        "id": "kCIgz0QonaSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SIZE = 32\n",
        "EMBED_DIM = 32\n",
        "ATTENTION_HEADS = 4\n",
        "BATCH_SIZE = 12\n",
        "EPOCHS = 10\n",
        "LR = 3e-4\n",
        "TEMPERATURE = 0.07"
      ],
      "metadata": {
        "id": "um56Y6_FnMlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Synthetic Dataset"
      ],
      "metadata": {
        "id": "UOcHtWXxnbju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"gray\"]\n",
        "shapes = [\"square\", \"circle\", \"triangle\"]\n",
        "positions = [\"left\", \"center\", \"right\", \"top\", \"bottom\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\"]"
      ],
      "metadata": {
        "id": "GwhfZ_B5n4Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Drawing image shapes"
      ],
      "metadata": {
        "id": "YG9gOPgsoXHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_sample(color, shape, position, img_size=IMG_SIZE):\n",
        "  img = Image.new(\"RGB\", (img_size, img_size), \"white\")\n",
        "  draw = ImageDraw.Draw(img)\n",
        "  margin = 6\n",
        "  w = h = img_size - 2 * margin\n",
        "\n",
        "  # Calculate the coordinates\n",
        "  if \"left\" in position:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif \"top-left\" in position:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif \"bottom-left\" in position:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif \"right\" in position:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  elif \"top-right\" in position:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  elif \"bottom-right\" in position:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  else:\n",
        "    x0 = margin + w // 4\n",
        "    x1 = margin + h // 2\n",
        "\n",
        "\n",
        "  # Calculate y coordinates\n",
        "  if \"top\" in position:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif \"top-left\" in position:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif \"top-right\" in position:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif \"bottom\" in position:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  elif \"bottom-left\" in position:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  elif \"bottom-right\" in position:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  else:\n",
        "    y0 = margin + h // 4\n",
        "    y1 = margin + 3 * h // 4\n",
        "\n",
        "  if shape == \"square\":\n",
        "    draw.rectangle([x0, y0, x1, y1], fill=color, outline=\"black\")\n",
        "  elif shape == \"circle\":\n",
        "    draw.ellipse([x0, y0, x1, y1], fill=color, outline=\"black\")\n",
        "  else:\n",
        "    draw.polygon([((x1+x0)//2, y0), (x0, y1), (x1, y1)], fill=color, outline=\"black\")\n",
        "\n",
        "  return img"
      ],
      "metadata": {
        "id": "2MgQE8dnoaGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Class for building the dataset"
      ],
      "metadata": {
        "id": "xExlhdpqsq0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShapesDataset():\n",
        "  def __init__(self):\n",
        "    self.images = []\n",
        "    self.captions = []\n",
        "\n",
        "    for c in colors:\n",
        "      for s in shapes:\n",
        "        for p in positions:\n",
        "          img = draw_sample(c, s, p)\n",
        "          cap = f\"{c} {s} {p}\"\n",
        "          self.images.append(torch.from_numpy(np.asarray(img)).permute(2,0,1).float()/255.0)\n",
        "          self.captions.append(cap)\n",
        "\n",
        "    self.vocab, self.word2idx = self.build_vocab(self.captions)\n",
        "\n",
        "  def build_vocab(self, texts):\n",
        "    words = sorted({w for t in texts for w in t.split()})\n",
        "    vocab = [\"[CLS]\"] + words\n",
        "    w2i = {w:i for i,w in enumerate(vocab)}\n",
        "    return vocab, w2i\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def encode_text(self, text):\n",
        "    toks = [self.word2idx[\"[CLS]\"]] + [self.word2idx[w] for w in text.split()]\n",
        "    return torch.tensor(toks, dtype=torch.long)\n",
        "\n",
        "  def __getitem__(self,  idx):\n",
        "    return self.images[idx], self.encode_text(self.captions[idx]), self.captions[idx]"
      ],
      "metadata": {
        "id": "pHAkBat_soYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Dataset"
      ],
      "metadata": {
        "id": "uJmkqMW0c5RC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_ds = ShapesDataset()\n",
        "VOCAB_SIZE = len(full_ds.vocab)\n",
        "print(VOCAB_SIZE)\n",
        "print(full_ds.vocab)"
      ],
      "metadata": {
        "id": "cldBZB8YeCMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train - Val data creation"
      ],
      "metadata": {
        "id": "2HQlSTvMeJHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(full_ds))\n",
        "val_size = len(full_ds) - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])"
      ],
      "metadata": {
        "id": "Sj73IfZXzcIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "rSqDpsIVzhRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "BrttDOcl0xpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Display a simple data point"
      ],
      "metadata": {
        "id": "tVq8VDTc1FgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, encoded_caps, _ = next(iter(train_loader))\n",
        "idx = random.randint(0, len(imgs) - 1)\n",
        "img = (imgs[idx].permute(1,2,0).numpy() * 255).astype(np.uint8) # Convert to displayable image\n",
        "\n",
        "# Decode the caption\n",
        "caption_tokens = encoded_caps[idx].tolist()\n",
        "caption = \"\".join([full_ds.vocab[token] for token in caption_tokens if token in range(len(full_ds.vocab))])\n",
        "# Remove the [CLS] token from the displayed caption\n",
        "caption = caption.replace(\"[CLS]\", \"\")\n",
        "\n",
        "plt.figure(figsize=(2.5, 2.5))\n",
        "plt.imshow(img)\n",
        "plt.title(caption, fontsize=8)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r5Fptwtt1QzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Image Encoder"
      ],
      "metadata": {
        "id": "WhLqAIJ519IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim = EMBED_DIM):\n",
        "    super().__init__()\n",
        "    self.convolutions = nn.Sequential(\n",
        "      nn.Conv2d(3, 32, 3, 2, 1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 64, 3, 2, 1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 128, 3, 2, 1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(128, 256, 3, 2, 1)\n",
        "    )\n",
        "\n",
        "    self.projection = nn.Linear(256, embed_dim)\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolutions(x)\n",
        "    x = x.mean(dim=[2,3])\n",
        "    x = self.projection(x)\n",
        "    x = F.normalize(self.norm(x), dim=-1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "djlkDs8Y4yXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Text Encoder"
      ],
      "metadata": {
        "id": "wnUNAqXMqBWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim = EMBED_DIM, num_heads = ATTENTION_HEADS, vocab_size = VOCAB_SIZE, context_window = 4):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.position_embedding = nn.Embedding(context_window, embed_dim)\n",
        "    self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "    self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, toks):\n",
        "   N, L = toks.shape\n",
        "   position_embedding = torch.arange(L, device=toks.device).unsqueeze(0).expand(N, L)\n",
        "   final_embedding = self.token_embedding(toks) + self.position_embedding(position_embedding)\n",
        "   context_vectors = self.mha(final_embedding, final_embedding, final_embedding)[0]\n",
        "   final_token = context_vectors[:,0]\n",
        "   projection = self.projection(final_token)\n",
        "   output = F.normalize(self.norm(projection), dim=-1)\n",
        "   return output"
      ],
      "metadata": {
        "id": "UIVZjdFz-KUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CLIP loss"
      ],
      "metadata": {
        "id": "L8Xp8T2sCquC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_loss(img_emb, txt_emb, temperature = TEMPERATURE):\n",
        "  logits = img_emb @ txt_emb.t()\n",
        "  targets = torch.arange(img_emb.size(0), device = img_emb.device)\n",
        "  loss_i = F.cross_entropy(logits, targets)\n",
        "  loss_t = F.cross_entropy(logits.t(), targets)\n",
        "  return ((loss_i + loss_t) / 2.0)"
      ],
      "metadata": {
        "id": "FkjEaCoUCsCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Model, data, optimizer"
      ],
      "metadata": {
        "id": "yakCjvihS6EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(full_ds.vocab)\n",
        "img_enc = ImageEncoder().to(device)\n",
        "txt_enc = TextEncoder().to(device)\n",
        "params = list(img_enc.parameters()) + list(txt_enc.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=LR)"
      ],
      "metadata": {
        "id": "LTrENFfYTNjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Before training embeddings"
      ],
      "metadata": {
        "id": "qmZHPua6Thvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(t, title=None):\n",
        "  img = (t.permute(1,2,0).numpy()*255).astype(np.uint8)\n",
        "  plt.figure(figsize=(2.2,2.2))\n",
        "  plt.axis(\"off\")\n",
        "  if title: plt.title(title, fontsize=8)\n",
        "  plt.imshow(img); plt.show()"
      ],
      "metadata": {
        "id": "TYMiEyrfVjga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_enc.eval(); txt_enc.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Select a random index\n",
        "  random_idx = random.randrange(len(full_ds))\n",
        "  sample_img, sample_toks, sample_cap = full_ds[random_idx]\n",
        "  sample_img = sample_img.unsqueeze(0).to(device)\n",
        "  sample_toks = sample_toks.unsqueeze(0).to(device)\n",
        "  pre_train_img_emb = img_enc(sample_img).squeeze(0).cpu().numpy()\n",
        "  pre_train_txt_emb = txt_enc(sample_toks).squeeze(0).cpu().numpy()\n",
        "\n",
        "# Display the same image and caption\n",
        "print(f\"Sample image and caption for embeddings visualization: '{sample_cap}'\")\n",
        "show_image(sample_img.squeeze(0).cpu())\n",
        "\n",
        "def plot_embedding(embedding, title):\n",
        "  plt.figure(figsize=(8,1))\n",
        "  plt.imshow(embedding.reshape(1,-1), aspect=\"auto\", cmap=\"viridis\")\n",
        "  plt.title(title)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "plot_embedding(pre_train_img_emb, \"Pre-Training Image Embedding\")\n",
        "plot_embedding(pre_train_txt_emb, \"Pre-Training Text Embedding\")"
      ],
      "metadata": {
        "id": "R5cq2qLuT0cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  ## Training loop"
      ],
      "metadata": {
        "id": "4B1r_R4XUf88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val = float(\"inf\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  img_enc.train(); txt_enc.train()\n",
        "  total = 0.0\n",
        "\n",
        "  for imgs, toks, _ in train_loader:\n",
        "    imgs = imgs.to(device); toks = toks.to(device)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    ie = img_enc(imgs); te = txt_enc(toks)\n",
        "    loss = clip_loss(ie, te)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total += loss.item() * imgs.size(0)\n",
        "  train_loss = total / (len(train_loader) * BATCH_SIZE)\n",
        "\n",
        "  # quick val\n",
        "  img_enc.eval(); txt_enc.eval()\n",
        "  with torch.no_grad():\n",
        "    vtotal, n = 0.0, 0\n",
        "    for imgs, toks, _ in val_loader:\n",
        "      imgs = imgs.to(device); toks = toks.to(device)\n",
        "      vtotal += clip_loss(img_enc(imgs), txt_enc(toks)).item()*imgs.size(0)\n",
        "      n += imgs.size(0)\n",
        "    val_loss = vtotal / n\n",
        "\n",
        "  print(f\"Epoch {epoch:02d} | train {train_loss:.4f} | val {val_loss:.4f}\")\n",
        "  best_val = min(best_val, val_loss)"
      ],
      "metadata": {
        "id": "KTVVuQZkjB3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Embeddings after training"
      ],
      "metadata": {
        "id": "CYVLe-XHlXVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_enc.eval(); txt_enc.eval()\n",
        "with torch.no_grad():\n",
        "  # use the same random index as before training\n",
        "  sample_img, sample_toks, sample_cap = full_ds[random_idx]\n",
        "  sample_img = sample_img.unsqueeze(0).to(device)\n",
        "  sample_toks = sample_toks.unsqueeze(0).to(device)\n",
        "\n",
        "  post_train_img_emb = img_enc(sample_img).squeeze(0).cpu().numpy()\n",
        "  post_train_txt_emb = txt_enc(sample_toks).squeeze(0).cpu().numpy()\n",
        "\n",
        "  # Display the sample image and caption\n",
        "  # Display the same image and caption\n",
        "print(f\"Sample image and caption for embeddings visualization: '{sample_cap}'\")\n",
        "show_image(sample_img.squeeze(0).cpu())\n",
        "\n",
        "plot_embedding(post_train_img_emb, \"Post-Training Image Embedding\")\n",
        "plot_embedding(post_train_txt_emb, \"Post-Training Text Embedding\")"
      ],
      "metadata": {
        "id": "vOJR9O_upeYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDot product between image and text embeddings:\")\n",
        "print(f\" Before training: {np.dot(pre_train_img_emb, pre_train_txt_emb):.4f}\")\n",
        "print(f\" After training: {np.dot(post_train_img_emb, post_train_txt_emb):.4f}\")"
      ],
      "metadata": {
        "id": "Cm91o2MuqEIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build text bank for retrieval on val set"
      ],
      "metadata": {
        "id": "LU792DXrur-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_enc.eval(); txt_enc.eval()\n",
        "with torch.no_grad():\n",
        "  val_imgs, val_toks, val_caps = [], [], []\n",
        "  for imgs, toks, caps in val_loader:\n",
        "    val_imgs.append(imgs); val_toks.append(toks); val_caps += list(caps)\n",
        "  val_imgs = torch.cat(val_imgs).to(device)\n",
        "  val_toks = torch.cat(val_toks).to(device)\n",
        "  img_emb = img_enc(val_imgs)\n",
        "  txt_emb = txt_enc(val_toks)"
      ],
      "metadata": {
        "id": "5ghX0dWiut6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Retriever helper functions"
      ],
      "metadata": {
        "id": "6nRjdUn_qerc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topk_text_for_images(k=3, idxs=None):\n",
        "  if idxs is None: idxs = np.random.choice(len(val_caps), size=1, replace=False)\n",
        "  sims = (img_emb @ txt_emb.t()).softmax(dim=1) # similarity as softmax\n",
        "  for i in idxs:\n",
        "    best = sims[i].topk(k).indices.tolist()\n",
        "    print(f\"\\nImage {i} best captions:\")\n",
        "    for j in best:\n",
        "      print(\"  -\", val_caps[j])\n",
        "    show_image(val_imgs[i].cpu())\n",
        "\n",
        "def topk_images_for_text(k=3, idxs=None):\n",
        "  if idxs is None: idxs = np.random.choice(len(val_caps), size=1, replace=False)\n",
        "  sims = (txt_emb @ img_emb.t()).softmax(dim=1)\n",
        "  for i in idxs:\n",
        "    best = sims[i].topk(k).indices.tolist()\n",
        "    print(f\"\\nText '{val_caps[i]}' best images:\")\n",
        "    for j in best:\n",
        "      show_image(val_imgs[j].cpu(), title=f\"match {val_caps[j]}\")"
      ],
      "metadata": {
        "id": "sDmbj9JxtaRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topk_text_for_images(k=1)\n",
        "topk_images_for_text(k=1)"
      ],
      "metadata": {
        "id": "b8Zj9p9nvK5G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}