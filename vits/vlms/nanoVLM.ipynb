{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMSS5XMMPpBMXNgMHWl6BD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vits/vlms/nanoVLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "diXQ9DkumHGB"
      },
      "outputs": [],
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math, random\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "rbUm1X6rmtnP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables"
      ],
      "metadata": {
        "id": "kCIgz0QonaSf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "IMG_SIZE = 32\n",
        "EMBED_DIM = 64\n",
        "ATTENTION_HEADS = 4\n",
        "BATCH_SIZE = 12\n",
        "EPOCHS = 10\n",
        "LR = 3e-4\n",
        "TEMPERATURE = 0.07"
      ],
      "metadata": {
        "id": "um56Y6_FnMlH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Synthetic Dataset"
      ],
      "metadata": {
        "id": "UOcHtWXxnbju"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"purple\", \"orange\", \"pink\", \"brown\", \"gray\"]\n",
        "shapes = [\"square\", \"circle\", \"triangle\"]\n",
        "positions = [\"left\", \"center\", \"right\", \"top\", \"bottom\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\"]"
      ],
      "metadata": {
        "id": "GwhfZ_B5n4Tc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Drawing image shapes"
      ],
      "metadata": {
        "id": "YG9gOPgsoXHB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_sample(color, shape, position, img_size=IMG_SIZE):\n",
        "  img = Image.new(\"RGB\", (img_size, img_size), \"white\")\n",
        "  draw = ImageDraw.Draw(img)\n",
        "  margin = 6\n",
        "  w = h = img_size - 2 * margin\n",
        "\n",
        "  # Calculate the coordinates\n",
        "  if \"left\" in position:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif \"top-left\" in position:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif \"bottom-left\" in position:\n",
        "    x0 = margin\n",
        "    x1 = margin + w // 2\n",
        "  elif \"right\" in position:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  elif \"top-right\" in position:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  elif \"bottom-right\" in position:\n",
        "    x0 = margin + w // 2\n",
        "    x1 = img_size - margin\n",
        "  else:\n",
        "    x0 = margin + w // 4\n",
        "    x1 = margin + h // 2\n",
        "\n",
        "\n",
        "  # Calculate y coordinates\n",
        "  if \"top\" in position:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif \"top-left\" in position:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif \"top-right\" in position:\n",
        "    y0 = margin\n",
        "    y1 = margin + h // 2\n",
        "  elif \"bottom\" in position:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  elif \"bottom-left\" in position:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  elif \"bottom-right\" in position:\n",
        "    y0 = margin + h // 2\n",
        "    y1 = img_size - margin\n",
        "  else:\n",
        "    y0 = margin + h // 4\n",
        "    y1 = margin + 3 * h // 4\n",
        "\n",
        "  if shape == \"square\":\n",
        "    draw.rectangle([x0, y0, x1, y1], fill=color, outline=\"black\")\n",
        "  elif shape == \"circle\":\n",
        "    draw.ellipse([x0, y0, x1, y1], fill=color, outline=\"black\")\n",
        "  else:\n",
        "    draw.polygon([((x1+x0)//2, y0), (x0, y1), (x1, y1)], fill=color, outline=\"black\")\n",
        "\n",
        "  return img\n",
        "\n"
      ],
      "metadata": {
        "id": "2MgQE8dnoaGy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Class for building the dataset"
      ],
      "metadata": {
        "id": "xExlhdpqsq0u"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShapesDataset():\n",
        "  def __init__(self):\n",
        "    self.images = []\n",
        "    self.captions = []\n",
        "\n",
        "    for c in colors:\n",
        "      for s in shapes:\n",
        "        for p in positions:\n",
        "          img = draw_sample(c, s, p)\n",
        "          cap = f\"{c} {s} {p}\"\n",
        "          self.images.append(torch.from_numpy(np.asarray(img)).permute(2,0,1).float()/255.0)\n",
        "          self.captions.append(cap)\n",
        "\n",
        "    self.vocab, self.word2idx = self.build_vocab(self.captions)\n",
        "\n",
        "  def build_vocab(self, texts):\n",
        "    words = sorted({w for t in texts for w in t.split()})\n",
        "    vocab = [\"[CLS]\"] + words\n",
        "    w2i = {w:i for i,w in enumerate(vocab)}\n",
        "    return vocab, w2i\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def encode_text(self, text):\n",
        "    toks = [self.word2idx[\"[CLS]\"]] + [self.word2idx[w] for w in text.split()]\n",
        "    return torch.tensor(toks, dtype=torch.long)\n",
        "\n",
        "  def __getitem__(self,  idx):\n",
        "    return self.images[idx], self.encode_text(self.captions[idx])"
      ],
      "metadata": {
        "id": "pHAkBat_soYD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Dataset"
      ],
      "metadata": {
        "id": "uJmkqMW0c5RC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_ds = ShapesDataset()\n",
        "VOCAB_SIZE = len(full_ds.vocab)\n",
        "print(VOCAB_SIZE)\n",
        "print(full_ds.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cldBZB8YeCMc",
        "outputId": "de3e1854-0f14-4b5a-f612-8083ddeba1ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n",
            "['[CLS]', 'blue', 'bottom', 'bottom-left', 'bottom-right', 'brown', 'center', 'circle', 'gray', 'green', 'left', 'orange', 'pink', 'purple', 'red', 'right', 'square', 'top', 'top-left', 'top-right', 'triangle', 'yellow']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3026023183.py:11: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  self.images.append(torch.from_numpy(np.asarray(img)).permute(2,0,1).float()/255.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Train - Val data creation"
      ],
      "metadata": {
        "id": "2HQlSTvMeJHN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(full_ds))\n",
        "val_size = len(full_ds) - train_size\n",
        "train_ds, val_ds = torch.utils.data.random_split(full_ds, [train_size, val_size])"
      ],
      "metadata": {
        "id": "Sj73IfZXzcIG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "rSqDpsIVzhRg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "BrttDOcl0xpM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Display a simple data point"
      ],
      "metadata": {
        "id": "tVq8VDTc1FgL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, encoded_caps = next(iter(train_loader))\n",
        "idx = random.randint(0, len(imgs) - 1)\n",
        "img = (imgs[idx].permute(1,2,0).numpy() * 255).astype(np.uint8) # Convert to displayable image\n",
        "\n",
        "# Decode the caption\n",
        "caption_tokens = encoded_caps[idx].tolist()\n",
        "caption = \"\".join([full_ds.vocab[token] for token in caption_tokens if token in range(len(full_ds.vocab))])\n",
        "# Remove the [CLS] token from the displayed caption\n",
        "caption = caption.replace(\"[CLS]\", \"\")\n",
        "\n",
        "plt.figure(figsize=(2.5, 2.5))\n",
        "plt.imshow(img)\n",
        "plt.title(caption, fontsize=8)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "r5Fptwtt1QzP",
        "outputId": "7ad83ac3-5a5d-498a-d8ce-0c1c82cd0787"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 250x250 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Image Encoder"
      ],
      "metadata": {
        "id": "WhLqAIJ519IS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.convolutions = nn.Sequential(\n",
        "      nn.Conv2d(3, 32, 3, 2, 1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 64, 3, 2, 1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 128, 3, 2, 1),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(128, 256, 3, 2, 1)\n",
        "    )\n",
        "\n",
        "    self.projection = nn.Linear(256, embed_dim)\n",
        "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.convolutions(x)\n",
        "    x = x.mean(dim=[2,3])\n",
        "    x = self.projection(x)\n",
        "    x = F.normalize(self.layernorm(x), dim=-1)\n",
        "    return x"
      ],
      "metadata": {
        "id": "djlkDs8Y4yXL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Text Encoder"
      ],
      "metadata": {
        "id": "wnUNAqXMqBWc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, embed_dim = EMBED_DIM, num_heads = ATTENTION_HEADS, vocab_size = VOCAB_SIZE, context_window = 4):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "    self.position_embedding = nn.Embedding(context_window, embed_dim)\n",
        "    self.mha = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "    self.projection = nn.Linear(embed_dim, embed_dim)\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, toks):\n",
        "   N, L = toks.shape\n",
        "   position_embedding = torch.arange(L, device=toks.device).unsequeeze(0).expand(N, L)\n",
        "   final_embedding = self.token_embedding(toks) + self.position_embedding(position_embedding)\n",
        "   context_vectors = self.mha(final_embedding, final_embedding, final_embedding)[0]\n",
        "   final_token = context_vectors[:,0]\n",
        "   projection = self.projection(final_token)\n",
        "   output = F.normalize(self.norm(projection), dim=-1)\n",
        "   return output"
      ],
      "metadata": {
        "id": "UIVZjdFz-KUL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CLIP loss"
      ],
      "metadata": {
        "id": "L8Xp8T2sCquC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkjEaCoUCsCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}