{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlcuNLEOydiR1C7KXLndmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vits/detection/DETR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Uvi68y1cOZE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW, lr_scheduler\n",
        "import torchvision.transforms as T\n",
        "from torchvision import datasets, ops\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from einops import rearrange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# DATA PREPARATION #\n",
        "####################"
      ],
      "metadata": {
        "id": "Rum2gmldoybL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lizhogn/tiny_coco_dataset.git"
      ],
      "metadata": {
        "id": "4KIBvacao4u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush', 'empty'\n",
        "]\n",
        "\n",
        "# Colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098],\n",
        "          [0.929, 0.694, 0.125], [0.494, 0.184, 0.556],\n",
        "          [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "COLORS *= 100\n",
        "\n",
        "\n",
        "revert_normalization = T.Normalize(\n",
        "    mean=[-.485/.229, -.456/.224, -.406/.225],\n",
        "    std=[1/.229, 1/.224, 1/.225]\n",
        ")\n",
        "\n",
        "\n",
        "def plot_im_with_boxes(im, boxes, probs=None, ax=None):\n",
        "    \"\"\"\n",
        "    Plot an image and render bounding boxes with optional labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    im : array-like or Tensor\n",
        "        The image to display in HWC format.\n",
        "    boxes : Tensor\n",
        "        Bounding boxes in xyxy format with shape (N, 4).\n",
        "    probs : Tensor, optional\n",
        "        If 1-D: contains class IDs for each box.\n",
        "        If 2-D: contains class probabilities per box (N, C).\n",
        "        If None: no labels are drawn.\n",
        "    ax : matplotlib.axes.Axes, optional\n",
        "        Existing axes on which to draw. If omitted, a new figure is created.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Uses COLORS to differentiate boxes and CLASSES to map class IDs\n",
        "    to human-readable labels.\n",
        "    \"\"\"\n",
        "\n",
        "    if ax is None:\n",
        "        plt.imshow(im)\n",
        "        ax = plt.gca()\n",
        "\n",
        "    for i, b in enumerate(boxes.tolist()):\n",
        "        xmin, ymin, xmax, ymax = b\n",
        "\n",
        "        patch = plt.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "            fill=False, color=COLORS[i], linewidth=2)\n",
        "\n",
        "        ax.add_patch(patch)\n",
        "        if probs is not None:\n",
        "            if probs.ndim == 1:\n",
        "                cl = probs[i].item()\n",
        "                text = f'{CLASSES[cl]}'\n",
        "            else:\n",
        "                cl = probs[i].argmax().item()\n",
        "                text = f'{CLASSES[cl]}: {probs[i,cl]:0.2f}'\n",
        "        else:\n",
        "            text = ''\n",
        "\n",
        "        ax.text(xmin, ymin, text, fontsize=7,\n",
        "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "\n",
        "def preprocess_target(anno, im_w, im_h):\n",
        "    \"\"\"\n",
        "    Convert COCO annotation dictionaries into normalized training targets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    anno : list of dict\n",
        "        Raw COCO annotations for a single image. Must contain \"bbox\"\n",
        "        in xywh format and \"category_id\".\n",
        "    im_w : int\n",
        "        Original image width in pixels.\n",
        "    im_h : int\n",
        "        Original image height in pixels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    classes : Tensor\n",
        "        Class IDs for valid bounding boxes.\n",
        "    boxes : Tensor\n",
        "        Bounding boxes in normalized cxcywh format.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Filters out annotations with \"iscrowd\" == 1.\n",
        "    - Converts xywh → xyxy.\n",
        "    - Removes invalid or degenerate boxes.\n",
        "    - Normalizes coordinates to [0, 1].\n",
        "    - Converts xyxy → cxcywh for downstream models.\n",
        "    \"\"\"\n",
        "\n",
        "    anno = [obj for obj in anno\n",
        "            if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "    boxes = [obj[\"bbox\"] for obj in anno]\n",
        "    boxes = torch.as_tensor(\n",
        "        boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "\n",
        "    # xywh -> xyxy\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    boxes[:, 0::2].clamp_(min=0, max=im_w)\n",
        "    boxes[:, 1::2].clamp_(min=0, max=im_h)\n",
        "    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "    boxes = boxes[keep]\n",
        "\n",
        "    classes = [obj[\"category_id\"] for obj in anno]\n",
        "    classes = torch.tensor(classes, dtype=torch.int64)\n",
        "    classes = classes[keep]\n",
        "\n",
        "    # scales boxes to [0,1]\n",
        "    boxes[:, 0::2] /= im_w\n",
        "    boxes[:, 1::2] /= im_h\n",
        "    boxes.clamp_(min=0, max=1)\n",
        "\n",
        "    boxes = ops.box_convert(boxes, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    return classes, boxes\n",
        "\n",
        "\n",
        "class MyCocoDetection(datasets.CocoDetection):\n",
        "    \"\"\"\n",
        "    A thin wrapper around torchvision.datasets.CocoDetection that applies\n",
        "    preprocessing transforms to images and annotations.\n",
        "\n",
        "    Adds:\n",
        "    - Image transforms: tensor conversion, normalization, fixed-size resize.\n",
        "    - Annotation preprocessing: COCO xywh → normalized cxcywh.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the dataset wrapper.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        *args, **kwargs : passed to CocoDetection\n",
        "            Standard initialization parameters such as the image root\n",
        "            directory and annotation file path.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.edge = 480\n",
        "\n",
        "        self.T = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[.485, .456, .406],\n",
        "                        std=[.229, .224, .225]),\n",
        "            T.Resize((self.edge, self.edge), antialias=True)\n",
        "        ])\n",
        "\n",
        "        self.T_target = preprocess_target\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetch an item and apply both image and annotation preprocessing.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Dataset index.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        image_tensor : Tensor\n",
        "            Preprocessed image of shape (3, edge, edge).\n",
        "        (classes, boxes) : tuple\n",
        "            Processed annotations where:\n",
        "            - classes: Tensor of class IDs\n",
        "            - boxes: Tensor of normalized cxcywh bounding boxes\n",
        "        \"\"\"\n",
        "        img, target = super().__getitem__(idx)\n",
        "        # PIL image\n",
        "        w, h = img.size\n",
        "\n",
        "        input_ = self.T(img)\n",
        "        classes, boxes = self.T_target(target, w, h)\n",
        "\n",
        "        return input_, (classes, boxes)\n",
        "\n",
        "\n",
        "def collate_fn(inputs):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to batch fixed-size images\n",
        "    while keeping variable-sized bounding-box annotations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs : list\n",
        "        Each element is (image_tensor, (classes, boxes)).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    batched_images : Tensor\n",
        "        A stacked tensor of images of shape (B, 3, H, W).\n",
        "    (classes, boxes) : tuple\n",
        "        Tuples of length B containing class tensors and box tensors\n",
        "        for each image.\n",
        "    \"\"\"\n",
        "    input_ = torch.stack([i[0] for i in inputs])\n",
        "    classes = tuple([i[1][0] for i in inputs])\n",
        "    boxes = tuple([i[1][1] for i in inputs])\n",
        "    return input_, (classes, boxes)\n"
      ],
      "metadata": {
        "id": "tSsM_9KGo6oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = MyCocoDetection(\n",
        "    'tiny_coco_dataset/tiny_coco/train2017/',\n",
        "    'tiny_coco_dataset/tiny_coco/annotations/instances_train2017.json',\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f'\\nNumber of training samples: {len(train_ds)}')\n",
        "# Number of training samples: 50"
      ],
      "metadata": {
        "id": "vLAncedYpZ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_, (target) = next(iter(train_loader))\n",
        "fig = plt.figure(figsize=(10, 10), constrained_layout=True)\n",
        "\n",
        "for ix in range(4):\n",
        "    t_cl = target[0][ix]\n",
        "    t_bbox = target[1][ix]\n",
        "\n",
        "    t_bbox = ops.box_convert(\n",
        "        t_bbox*480, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    im = revert_normalization(input_)[ix].\\\n",
        "        permute(1,2,0).cpu().clip(0,1)\n",
        "\n",
        "    ax = fig.add_subplot(2, 2, ix+1)\n",
        "    ax.imshow(im)\n",
        "    plot_im_with_boxes(im, t_bbox, t_cl, ax=ax)\n",
        "    ax.set_axis_off()"
      ],
      "metadata": {
        "id": "O8D_X734pcjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQxh1_VGpg01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygfe_jzmvnJq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}