{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFL1zf98E8ylNe4472VcDC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vits/detection/DETR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Uvi68y1cOZE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW, lr_scheduler\n",
        "import torchvision.transforms as T\n",
        "from torchvision import datasets, ops\n",
        "from torchvision.models.feature_extraction import create_feature_extractor\n",
        "from einops import rearrange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.optimize import linear_sum_assignment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################\n",
        "# DATA PREPARATION #\n",
        "####################"
      ],
      "metadata": {
        "id": "Rum2gmldoybL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lizhogn/tiny_coco_dataset.git"
      ],
      "metadata": {
        "id": "4KIBvacao4u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush', 'empty'\n",
        "]\n",
        "\n",
        "# Colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098],\n",
        "          [0.929, 0.694, 0.125], [0.494, 0.184, 0.556],\n",
        "          [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "COLORS *= 100\n",
        "\n",
        "\n",
        "revert_normalization = T.Normalize(\n",
        "    mean=[-.485/.229, -.456/.224, -.406/.225],\n",
        "    std=[1/.229, 1/.224, 1/.225]\n",
        ")\n",
        "\n",
        "\n",
        "def plot_im_with_boxes(im, boxes, probs=None, ax=None):\n",
        "    \"\"\"\n",
        "    Plot an image and render bounding boxes with optional labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    im : array-like or Tensor\n",
        "        The image to display in HWC format.\n",
        "    boxes : Tensor\n",
        "        Bounding boxes in xyxy format with shape (N, 4).\n",
        "    probs : Tensor, optional\n",
        "        If 1-D: contains class IDs for each box.\n",
        "        If 2-D: contains class probabilities per box (N, C).\n",
        "        If None: no labels are drawn.\n",
        "    ax : matplotlib.axes.Axes, optional\n",
        "        Existing axes on which to draw. If omitted, a new figure is created.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Uses COLORS to differentiate boxes and CLASSES to map class IDs\n",
        "    to human-readable labels.\n",
        "    \"\"\"\n",
        "\n",
        "    if ax is None:\n",
        "        plt.imshow(im)\n",
        "        ax = plt.gca()\n",
        "\n",
        "    for i, b in enumerate(boxes.tolist()):\n",
        "        xmin, ymin, xmax, ymax = b\n",
        "\n",
        "        patch = plt.Rectangle(\n",
        "            (xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "            fill=False, color=COLORS[i], linewidth=2)\n",
        "\n",
        "        ax.add_patch(patch)\n",
        "        if probs is not None:\n",
        "            if probs.ndim == 1:\n",
        "                cl = probs[i].item()\n",
        "                text = f'{CLASSES[cl]}'\n",
        "            else:\n",
        "                cl = probs[i].argmax().item()\n",
        "                text = f'{CLASSES[cl]}: {probs[i,cl]:0.2f}'\n",
        "        else:\n",
        "            text = ''\n",
        "\n",
        "        ax.text(xmin, ymin, text, fontsize=7,\n",
        "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "\n",
        "def preprocess_target(anno, im_w, im_h):\n",
        "    \"\"\"\n",
        "    Convert COCO annotation dictionaries into normalized training targets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    anno : list of dict\n",
        "        Raw COCO annotations for a single image. Must contain \"bbox\"\n",
        "        in xywh format and \"category_id\".\n",
        "    im_w : int\n",
        "        Original image width in pixels.\n",
        "    im_h : int\n",
        "        Original image height in pixels.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    classes : Tensor\n",
        "        Class IDs for valid bounding boxes.\n",
        "    boxes : Tensor\n",
        "        Bounding boxes in normalized cxcywh format.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - Filters out annotations with \"iscrowd\" == 1.\n",
        "    - Converts xywh → xyxy.\n",
        "    - Removes invalid or degenerate boxes.\n",
        "    - Normalizes coordinates to [0, 1].\n",
        "    - Converts xyxy → cxcywh for downstream models.\n",
        "    \"\"\"\n",
        "\n",
        "    anno = [obj for obj in anno\n",
        "            if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
        "\n",
        "    boxes = [obj[\"bbox\"] for obj in anno]\n",
        "    boxes = torch.as_tensor(\n",
        "        boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "\n",
        "    # xywh -> xyxy\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    boxes[:, 0::2].clamp_(min=0, max=im_w)\n",
        "    boxes[:, 1::2].clamp_(min=0, max=im_h)\n",
        "    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "    boxes = boxes[keep]\n",
        "\n",
        "    classes = [obj[\"category_id\"] for obj in anno]\n",
        "    classes = torch.tensor(classes, dtype=torch.int64)\n",
        "    classes = classes[keep]\n",
        "\n",
        "    # scales boxes to [0,1]\n",
        "    boxes[:, 0::2] /= im_w\n",
        "    boxes[:, 1::2] /= im_h\n",
        "    boxes.clamp_(min=0, max=1)\n",
        "\n",
        "    boxes = ops.box_convert(boxes, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    return classes, boxes\n",
        "\n",
        "\n",
        "class MyCocoDetection(datasets.CocoDetection):\n",
        "    \"\"\"\n",
        "    A thin wrapper around torchvision.datasets.CocoDetection that applies\n",
        "    preprocessing transforms to images and annotations.\n",
        "\n",
        "    Adds:\n",
        "    - Image transforms: tensor conversion, normalization, fixed-size resize.\n",
        "    - Annotation preprocessing: COCO xywh → normalized cxcywh.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the dataset wrapper.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        *args, **kwargs : passed to CocoDetection\n",
        "            Standard initialization parameters such as the image root\n",
        "            directory and annotation file path.\n",
        "        \"\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.edge = 480\n",
        "\n",
        "        self.T = T.Compose([\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[.485, .456, .406],\n",
        "                        std=[.229, .224, .225]),\n",
        "            T.Resize((self.edge, self.edge), antialias=True)\n",
        "        ])\n",
        "\n",
        "        self.T_target = preprocess_target\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetch an item and apply both image and annotation preprocessing.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Dataset index.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        image_tensor : Tensor\n",
        "            Preprocessed image of shape (3, edge, edge).\n",
        "        (classes, boxes) : tuple\n",
        "            Processed annotations where:\n",
        "            - classes: Tensor of class IDs\n",
        "            - boxes: Tensor of normalized cxcywh bounding boxes\n",
        "        \"\"\"\n",
        "        img, target = super().__getitem__(idx)\n",
        "        # PIL image\n",
        "        w, h = img.size\n",
        "\n",
        "        input_ = self.T(img)\n",
        "        classes, boxes = self.T_target(target, w, h)\n",
        "\n",
        "        return input_, (classes, boxes)\n",
        "\n",
        "\n",
        "def collate_fn(inputs):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader to batch fixed-size images\n",
        "    while keeping variable-sized bounding-box annotations.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    inputs : list\n",
        "        Each element is (image_tensor, (classes, boxes)).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    batched_images : Tensor\n",
        "        A stacked tensor of images of shape (B, 3, H, W).\n",
        "    (classes, boxes) : tuple\n",
        "        Tuples of length B containing class tensors and box tensors\n",
        "        for each image.\n",
        "    \"\"\"\n",
        "    input_ = torch.stack([i[0] for i in inputs])\n",
        "    classes = tuple([i[1][0] for i in inputs])\n",
        "    boxes = tuple([i[1][1] for i in inputs])\n",
        "    return input_, (classes, boxes)\n"
      ],
      "metadata": {
        "id": "tSsM_9KGo6oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = MyCocoDetection(\n",
        "    'tiny_coco_dataset/tiny_coco/train2017/',\n",
        "    'tiny_coco_dataset/tiny_coco/annotations/instances_train2017.json',\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f'\\nNumber of training samples: {len(train_ds)}')\n",
        "# Number of training samples: 50"
      ],
      "metadata": {
        "id": "vLAncedYpZ2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_, (target) = next(iter(train_loader))\n",
        "fig = plt.figure(figsize=(10, 10), constrained_layout=True)\n",
        "\n",
        "for ix in range(4):\n",
        "    t_cl = target[0][ix]\n",
        "    t_bbox = target[1][ix]\n",
        "\n",
        "    t_bbox = ops.box_convert(\n",
        "        t_bbox*480, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    im = revert_normalization(input_)[ix].\\\n",
        "        permute(1,2,0).cpu().clip(0,1)\n",
        "\n",
        "    ax = fig.add_subplot(2, 2, ix+1)\n",
        "    ax.imshow(im)\n",
        "    plot_im_with_boxes(im, t_bbox, t_cl, ax=ax)\n",
        "    ax.set_axis_off()"
      ],
      "metadata": {
        "id": "O8D_X734pcjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################\n",
        "# MODEL ARCHITECTURE #\n",
        "######################"
      ],
      "metadata": {
        "id": "iQxh1_VGpg01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = create_feature_extractor(\n",
        "    torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True),\n",
        "    return_nodes={'layer4': 'layer4'}\n",
        ")\n",
        "\n",
        "d_model = 256\n",
        "conv1x1 = nn.Conv2d(2048, d_model, kernel_size=1, stride=1)\n",
        "\n",
        "x = torch.randn((1, 3, 480, 480))\n",
        "embeddings = backbone(x)['layer4']\n",
        "embeddings = conv1x1(embeddings)\n",
        "\n",
        "print(x.shape, embeddings.shape)\n",
        "# torch.Size([1, 3, 480, 480]) torch.Size([1, 256, 15, 15])"
      ],
      "metadata": {
        "id": "ygfe_jzmvnJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Positional Embeddings"
      ],
      "metadata": {
        "id": "ocs3RFJRSbrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_old = embeddings.clone()\n",
        "embeddings = rearrange(embeddings, 'b c h w -> b (h w) c')\n",
        "\n",
        "print(embeddings_old.shape, embeddings.shape)\n",
        "# torch.Size([1, 256, 15, 15]) torch.Size([1, 225, 256])"
      ],
      "metadata": {
        "id": "FCflgoEnQKp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the original implementation uses sine spatial PE\n",
        "n_tokens = 225\n",
        "\n",
        "inp_pe = nn.Parameter(\n",
        "    torch.rand((1, n_tokens, d_model)),\n",
        "    requires_grad=False\n",
        ")\n",
        "\n",
        "embeddings += inp_pe\n",
        "\n",
        "print(inp_pe.shape, embeddings.shape)\n",
        "# torch.Size([1, 225, 256]) torch.Size([1, 225, 256])"
      ],
      "metadata": {
        "id": "zJIC6fCiO87g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "FvkXL_VmSZDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_heads = 8\n",
        "encoder_layer = nn.TransformerEncoderLayer(\n",
        "    d_model=d_model,\n",
        "    nhead=n_heads,\n",
        "    dim_feedforward=4*d_model,\n",
        "    batch_first=True\n",
        ")\n",
        "\n",
        "n_layers = 6\n",
        "transformer_encoder = nn.TransformerEncoder(\n",
        "    encoder_layer, num_layers=n_layers)\n",
        "\n",
        "out_encoder = transformer_encoder(embeddings)\n",
        "\n",
        "print(out_encoder.shape)\n",
        "# torch.Size([1, 225, 256])"
      ],
      "metadata": {
        "id": "vRfN1UNNQDoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "kbpcHCMmQydU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_queries = 100\n",
        "\n",
        "queries = nn.Parameter(\n",
        "    torch.rand((1, n_queries, d_model)),\n",
        "    requires_grad=False\n",
        ")\n",
        "\n",
        "print(queries.shape, embeddings.shape)\n",
        "# torch.Size([1, 100, 256]) torch.Size([1, 225, 256])"
      ],
      "metadata": {
        "id": "oIvefmQnSPVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_layer = nn.TransformerDecoderLayer(\n",
        "    d_model=d_model,\n",
        "    nhead=n_heads,\n",
        "    dim_feedforward=4*d_model,\n",
        "    batch_first=True\n",
        ")\n",
        "\n",
        "transformer_decoder = nn.TransformerDecoder(\n",
        "    decoder_layer, num_layers=n_layers)\n",
        "\n",
        "out_decoder = transformer_decoder(queries, out_encoder)\n",
        "\n",
        "print(out_decoder.shape, queries.shape, out_encoder.shape)\n",
        "# torch.Size([1, 100, 256]) torch.Size([1, 100, 256]) torch.Size([1, 225, 256])"
      ],
      "metadata": {
        "id": "qFF9FhgYSSIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Feed Forward"
      ],
      "metadata": {
        "id": "5Fug1iahSUCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 92\n",
        "\n",
        "linear_class = nn.Linear(d_model, n_classes)\n",
        "linear_bbox = nn.Linear(d_model, 4)\n",
        "\n",
        "pred_classes = linear_class(out_decoder)\n",
        "pred_bboxes = linear_bbox(out_decoder)\n",
        "\n",
        "print(pred_classes.shape, pred_bboxes.shape)\n",
        "# torch.Size([1, 100, 92]) torch.Size([1, 100, 4])"
      ],
      "metadata": {
        "id": "ODnZcrO0TuaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hook(outs, name):\n",
        "    def hook(self, input, output):\n",
        "        outs[name] = output\n",
        "    return hook\n",
        "\n",
        "\n",
        "class DETR(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, d_model=256, n_classes=92, n_tokens=225,\n",
        "        n_layers=6, n_heads=8, n_queries=100\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = create_feature_extractor(\n",
        "            torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True),\n",
        "            return_nodes={'layer4': 'layer4'}\n",
        "        )\n",
        "\n",
        "        self.conv1x1 = nn.Conv2d(\n",
        "            2048, d_model, kernel_size=1, stride=1)\n",
        "\n",
        "        self.pe_encoder = nn.Parameter(\n",
        "            torch.rand((1, n_tokens, d_model)),\n",
        "            requires_grad=True\n",
        "        )\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=4*d_model,\n",
        "            dropout=0.1,\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.queries = nn.Parameter(\n",
        "            torch.rand((1, n_queries, d_model)),\n",
        "            requires_grad=True\n",
        "        )\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=4*d_model,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.transformer_decoder = nn.TransformerDecoder(\n",
        "            decoder_layer, num_layers=n_layers)\n",
        "\n",
        "        self.linear_class = nn.Linear(d_model, n_classes)\n",
        "        self.linear_bbox = nn.Linear(d_model, 4)\n",
        "\n",
        "        # Add hooks to get intermediate outcomes\n",
        "        self.decoder_outs = {}\n",
        "        for i, L in enumerate(self.transformer_decoder.layers):\n",
        "            name = f'layer_{i}'\n",
        "            L.register_forward_hook(\n",
        "                get_hook(self.decoder_outs, name))\n",
        "\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        def freeze_batchnorm(module):\n",
        "            # Switch to evaluation mode (uses running stats)\n",
        "            if isinstance(module, nn.BatchNorm2d):\n",
        "                module.eval()\n",
        "                for param in module.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        super().train(mode=mode)\n",
        "        self.backbone.apply(freeze_batchnorm)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tokens = self.backbone(x)['layer4']\n",
        "        tokens = self.conv1x1(tokens)\n",
        "        tokens = rearrange(tokens, 'b c h w -> b (h w) c')\n",
        "\n",
        "        out_encoder = self.transformer_encoder(\n",
        "            tokens + self.pe_encoder)\n",
        "\n",
        "        out_decoder = self.transformer_decoder(\n",
        "            self.queries.repeat(len(out_encoder), 1, 1),\n",
        "            out_encoder)\n",
        "\n",
        "        # Compute outcomes for all intermediate\n",
        "        # decoder's layers\n",
        "        outs = {}\n",
        "        for n, o in self.decoder_outs.items():\n",
        "            outs[n] = {\n",
        "                'cl': self.linear_class(o),\n",
        "                'bbox': self.linear_bbox(o)\n",
        "            }\n",
        "\n",
        "        return outs"
      ],
      "metadata": {
        "id": "iExsZaNETupo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detr = DETR(\n",
        "    d_model=256, n_classes=92, n_tokens=225,\n",
        "    n_layers=6, n_heads=8, n_queries=100\n",
        ")\n",
        "\n",
        "x = torch.randn((1, 3, 480, 480))\n",
        "outs = detr(x)\n",
        "pred_cl, pred_boxes = outs['layer_5'].values()\n",
        "\n",
        "print(pred_cl.shape, pred_boxes.shape)\n",
        "# torch.Size([1, 100, 92]) torch.Size([1, 100, 4])"
      ],
      "metadata": {
        "id": "5Qj3Z2fOT8uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "input_, (tgt_cl, tgt_bbox) = next(iter(train_loader))\n",
        "\n",
        "outs = detr(input_)\n",
        "out_cl, out_bbox = outs['layer_5'].values()\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "input_, (tgt_cl, tgt_bbox) = next(iter(train_loader))\n",
        "\n",
        "outs = detr(input_)\n",
        "out_cl, out_bbox = outs['layer_5'].values()\n",
        "\n",
        "out_bbox = out_bbox.sigmoid()\n",
        "\n",
        "o_bbox = out_bbox[0]\n",
        "t_bbox = tgt_bbox[0]\n",
        "o_cl = out_cl[0]\n",
        "t_cl = tgt_cl[0]\n",
        "\n",
        "o_probs = o_cl.softmax(dim=-1)\n",
        "\n",
        "# Negative sign here because we want the maximum magnitude\n",
        "C_classes = -o_probs[..., t_cl]\n",
        "\n",
        "# Positive sign here because we want to shrink the l1-norm\n",
        "C_boxes = torch.cdist(o_bbox, t_bbox, p=1)\n",
        "\n",
        "# Negative sign here because we want the maximum magnitude\n",
        "C_giou = -ops.generalized_box_iou(\n",
        "    ops.box_convert(o_bbox, in_fmt='cxcywh', out_fmt='xyxy'),\n",
        "    ops.box_convert(t_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
        ")\n",
        "\n",
        "C_total = 1*C_classes + 5*C_boxes + 2*C_giou\n",
        "\n",
        "# Convert the tensor to numpy array\n",
        "C_total = C_total.cpu().detach().numpy()\n",
        "\n",
        "# Find the optimum pairs that produces the minimum summation.\n",
        "# The method returns the pair indices\n",
        "o_ixs, t_ixs = linear_sum_assignment(C_total)\n",
        "\n",
        "print(t_bbox.shape, o_bbox.shape, C_total.shape)\n",
        "out_bbox = out_bbox.sigmoid()\n",
        "\n",
        "o_bbox = out_bbox[0]\n",
        "t_bbox = tgt_bbox[0]\n",
        "o_cl = out_cl[0]\n",
        "t_cl = tgt_cl[0]\n",
        "\n",
        "o_probs = o_cl.softmax(dim=-1)\n",
        "\n",
        "# Negative sign here because we want the maximum magnitude\n",
        "C_classes = -o_probs[..., t_cl]\n",
        "\n",
        "# Positive sign here because we want to shrink the l1-norm\n",
        "C_boxes = torch.cdist(o_bbox, t_bbox, p=1)\n",
        "\n",
        "# Negative sign here because we want the maximum magnitude\n",
        "C_giou = -ops.generalized_box_iou(\n",
        "    ops.box_convert(o_bbox, in_fmt='cxcywh', out_fmt='xyxy'),\n",
        "    ops.box_convert(t_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
        ")\n",
        "\n",
        "C_total = 1*C_classes + 5*C_boxes + 2*C_giou\n",
        "\n",
        "# Convert the tensor to numpy array\n",
        "C_total = C_total.cpu().detach().numpy()\n",
        "\n",
        "# Find the optimum pairs that produces the minimum summation.\n",
        "# The method returns the pair indices\n",
        "o_ixs, t_ixs = linear_sum_assignment(C_total)\n",
        "\n",
        "print(t_bbox.shape, o_bbox.shape, C_total.shape)"
      ],
      "metadata": {
        "id": "UR0sXmq2T-8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform indices to tensors\n",
        "o_ixs = torch.IntTensor(o_ixs)\n",
        "t_ixs = torch.IntTensor(t_ixs)\n",
        "\n",
        "# Reorder o_ixs to naturally align with target_cl length, such\n",
        "# the pairs are {(o_ixs[0], t[0]), {o_ixs[1], t[1]}, ...}\n",
        "o_ixs = o_ixs[t_ixs.argsort()]\n",
        "\n",
        "# Average over the number of boxes, not the number of coordinates\n",
        "num_boxes = len(t_bbox)\n",
        "loss_bbox = F.l1_loss(\n",
        "    o_bbox[o_ixs], t_bbox, reduce='sum') / num_boxes\n",
        "\n",
        "# Vectorize the operation\n",
        "target_gIoU = ops.generalized_box_iou(\n",
        "    ops.box_convert(o_bbox[o_ixs], in_fmt='cxcywh', out_fmt='xyxy'),\n",
        "    ops.box_convert(t_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
        ")\n",
        "# Get only the matrix diagonal that contains the bipartite pairs\n",
        "# and transform gIoU into a loss\n",
        "loss_giou = 1 - torch.diag(target_gIoU).mean()\n",
        "\n",
        "# Assign empty class for the outside predictions\n",
        "queries_classes_label = torch.full(o_probs.shape[:1], 91)\n",
        "queries_classes_label[o_ixs] = t_cl\n",
        "loss_class = F.cross_entropy(o_cl, queries_classes_label)\n",
        "\n",
        "loss_total = 1*loss_class + 5*loss_bbox + 2*loss_giou\n",
        "print(loss_total)\n",
        "# tensor(5.5788, grad_fn=<AddBackward0>)"
      ],
      "metadata": {
        "id": "pKndatU9iNSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sample_loss(\n",
        "    o_bbox, t_bbox, o_cl, t_cl, n_queries=100):\n",
        "\n",
        "    # If  the example has none box, we just feed it with empty classes.\n",
        "    if len(t_cl) > 0:\n",
        "\n",
        "        t_bbox = t_bbox.cuda()\n",
        "        t_cl = t_cl.cuda()\n",
        "\n",
        "        o_probs = o_cl.softmax(dim=-1)\n",
        "\n",
        "        # Negative sign here because we want the maximum magnitude\n",
        "        C_classes = -o_probs[..., t_cl]\n",
        "\n",
        "        # Positive sign here because we want to shrink the l1-norm\n",
        "        C_boxes = torch.cdist(o_bbox, t_bbox, p=1)\n",
        "\n",
        "        # Negative sign here because we want the maximum magnitude\n",
        "        C_giou = -ops.generalized_box_iou(\n",
        "            ops.box_convert(o_bbox, in_fmt='cxcywh', out_fmt='xyxy'),\n",
        "            ops.box_convert(t_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "        )\n",
        "\n",
        "        C_total = 1*C_classes + 5*C_boxes + 2*C_giou\n",
        "\n",
        "        # Convert the tensor to numpy array\n",
        "        C_total = C_total.cpu().detach().numpy()\n",
        "\n",
        "        # Find the optimum pairs that produces the minimum summation.\n",
        "        # the method returns the pair indices\n",
        "        o_ixs, t_ixs = linear_sum_assignment(C_total)\n",
        "\n",
        "        # Transform indices to tensors\n",
        "        o_ixs = torch.IntTensor(o_ixs)\n",
        "        t_ixs = torch.IntTensor(t_ixs)\n",
        "\n",
        "        # Reorder o_ixs to naturally align with target_cl length, such\n",
        "        # the pairs are {(o_ixs[0], t[0]), {o_ixs[1], t[1]}, ...}\n",
        "        o_ixs = o_ixs[t_ixs.argsort()]\n",
        "\n",
        "        # Average over the number of boxes, not the number of coordinates\n",
        "        num_boxes = len(t_bbox)\n",
        "        loss_bbox = F.l1_loss(\n",
        "            o_bbox[o_ixs], t_bbox, reduce='sum') / num_boxes\n",
        "\n",
        "        # vectorize the operation\n",
        "        target_gIoU = ops.generalized_box_iou(\n",
        "            ops.box_convert(o_bbox[o_ixs], in_fmt='cxcywh', out_fmt='xyxy'),\n",
        "            ops.box_convert(t_bbox, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "        )\n",
        "        # get only the matrix diagonal that contains the bipartite pairs\n",
        "        # and transform gIoU into a loss\n",
        "        loss_giou = 1 - torch.diag(target_gIoU).mean()\n",
        "\n",
        "        # assign empty class for the outside predictions\n",
        "        queries_classes_label = torch.full(o_probs.shape[:1], 91).cuda()\n",
        "        queries_classes_label[o_ixs] = t_cl\n",
        "        loss_class = F.cross_entropy(o_cl, queries_classes_label)\n",
        "\n",
        "    else:\n",
        "        queries_classes_label = torch.full((n_queries,), 91).cuda()\n",
        "        loss_class = F.cross_entropy(o_cl, queries_classes_label)\n",
        "        loss_bbox = loss_giou = torch.tensor(0)\n",
        "\n",
        "    return loss_class, loss_bbox, loss_giou"
      ],
      "metadata": {
        "id": "uwIqyVNPiP76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure saving directory exists\n",
        "![ ! -d ckpts-tiny ] && mkdir ckpts-tiny\n",
        "\n",
        "detr = DETR(d_model=256, n_classes=92, n_tokens=225,\n",
        "            n_layers=6, n_heads=8, n_queries=100)\n",
        "detr.cuda()\n",
        "\n",
        "backbone_params = [\n",
        "    p for n, p in detr.named_parameters() if 'backbone.' in n]\n",
        "\n",
        "for p in detr.backbone.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "transformer_params = [\n",
        "    p for n, p in detr.named_parameters() if 'backbone.' not in n]\n",
        "\n",
        "optimizer = AdamW([\n",
        "    {'params': transformer_params, 'lr': 1e-5},\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "\n",
        "nparams = sum([p.nelement() for p in detr.parameters()]) / 1e6\n",
        "print(f'DETR params: {nparams:.1f}M')\n",
        "# DETR params: 35.2M"
      ],
      "metadata": {
        "id": "C-Qmm9hyiS-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(True)\n",
        "detr.train()\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(\n",
        "    train_ds, batch_size=batch_size,\n",
        "    shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "niters = 4000\n",
        "print_every_n = 100\n",
        "save_every_n = 1000\n",
        "losses = []\n",
        "\n",
        "hist = []\n",
        "iters = 1\n",
        "\n",
        "\n",
        "while iters <= niters:\n",
        "\n",
        "    for input_, (tgt_cl, tgt_bbox) in train_loader:\n",
        "        input_ = input_.cuda()\n",
        "\n",
        "        outs = detr(input_)\n",
        "\n",
        "        loss = torch.Tensor([0]).cuda()\n",
        "        for name, out in outs.items():\n",
        "\n",
        "            out['bbox'] = out['bbox'].sigmoid()\n",
        "\n",
        "            for o_bbox, t_bbox, o_cl, t_cl in zip(\n",
        "                out['bbox'], tgt_bbox, out['cl'], tgt_cl):\n",
        "\n",
        "                loss_class, loss_bbox, loss_giou = compute_sample_loss(\n",
        "                    o_bbox, t_bbox, o_cl, t_cl)\n",
        "\n",
        "                sample_loss = 1*loss_class + 5*loss_bbox + 2*loss_giou\n",
        "\n",
        "                loss += sample_loss / batch_size / len(outs)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "\n",
        "        # clip gradient norms\n",
        "        nn.utils.clip_grad_norm_(detr.parameters(), .1)\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if iters % print_every_n == 0:\n",
        "            loss_avg = np.mean(losses[-10:])\n",
        "            print_text = f'iters: {iters},\\tloss: {loss_avg:.4f}'\n",
        "            print(print_text)\n",
        "            print(f'loss_class: {loss_class.item():.4f}\\tloss_bbox: {loss_bbox.item():.4f}\\tloss_giou: {loss_giou.item():.4f}\\t')\n",
        "\n",
        "            hist.append(loss_avg)\n",
        "            losses = []\n",
        "\n",
        "        if iters % save_every_n == 0 and iters > 0:\n",
        "            str_iters = str(iters)\n",
        "            str_iters = '0'*(6-len(str_iters)) + str_iters\n",
        "            torch.save(detr.state_dict(), f'ckpts-tiny/model_it{str_iters}.pt')\n",
        "            np.save(f'ckpts-tiny/hist_it{str_iters}.npy', hist)\n",
        "\n",
        "        iters += 1\n",
        "        if iters > niters:\n",
        "            break"
      ],
      "metadata": {
        "id": "Ek13K_ohiU1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2K-CbTjimxe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}