{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vits/classification/ViT_classification(bounding_boxes).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU1iYAgt1iCM"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3RoFmzu7SNp"
      },
      "outputs": [],
      "source": [
        "###############################\n",
        "# DATASET WITH BOUNDING BOXES #\n",
        "###############################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGzOlh3J7aKX"
      },
      "outputs": [],
      "source": [
        "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6JPADuP74_d"
      },
      "outputs": [],
      "source": [
        "!unzip PennFudanPed.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import timeit\n",
        "import math\n",
        "import numpy\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torchinfo import summary\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from dataclasses import dataclass\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "T2oQAJQDinXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdcEkuLh8R6k"
      },
      "outputs": [],
      "source": [
        "class PennFudanSinglePed(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "\n",
        "        self.img_dir = os.path.join(root, \"PNGImages\")\n",
        "        self.mask_dir = os.path.join(root, \"PedMasks\")\n",
        "\n",
        "        self.images = sorted(os.listdir(self.img_dir))\n",
        "        self.masks = sorted(os.listdir(self.mask_dir))\n",
        "\n",
        "        self.valid_indices = []\n",
        "        for idx in range(len(self.images)):\n",
        "            mask = np.array(Image.open(os.path.join(self.mask_dir, self.masks[idx])))\n",
        "            obj_ids = np.unique(mask)\n",
        "            obj_ids = obj_ids[obj_ids != 0]\n",
        "            if len(obj_ids) == 1:\n",
        "                self.valid_indices.append(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.valid_indices[idx]\n",
        "\n",
        "        img_path  = os.path.join(self.img_dir,  self.images[real_idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[real_idx])\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = np.array(Image.open(mask_path))\n",
        "\n",
        "        orig_w, orig_h = img.size\n",
        "\n",
        "        ys, xs = np.where(mask == 1)\n",
        "        xmin, xmax = xs.min(), xs.max()\n",
        "        ymin, ymax = ys.min(), ys.max()\n",
        "\n",
        "        # Normalize BEFORE any transform\n",
        "        bbox = torch.tensor([\n",
        "            xmin / orig_w,\n",
        "            ymin / orig_h,\n",
        "            xmax / orig_w,\n",
        "            ymax / orig_h\n",
        "        ], dtype=torch.float32)\n",
        "\n",
        "        label = torch.tensor(1)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return {\"image\": img, \"label\": label, \"bbox\": bbox}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uujDLiXJ8WuK"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = PennFudanSinglePed(\n",
        "    root=\"PennFudanPed\",\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for dictionary in loader:\n",
        "    print(dictionary[\"image\"].shape)\n",
        "    print(dictionary[\"label\"])\n",
        "    print(dictionary[\"bbox\"])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o95UF5CkRxSL"
      },
      "outputs": [],
      "source": [
        "#################\n",
        "# CONFIGURATION #\n",
        "#################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMKB7wcj1a5S"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class vit_config:\n",
        "    num_channels: int = 3\n",
        "    batch_size:int = 16\n",
        "    image_size: int = 224\n",
        "    patch_size: int = 16\n",
        "    num_heads:int = 8\n",
        "    dropout: float = 0.0\n",
        "    layer_norm_eps: float = 1e-6\n",
        "    num_encoder_layers: int = 12\n",
        "    random_seed: int = 42\n",
        "    epochs: int = 30\n",
        "    num_classes: int = 10\n",
        "    learning_rate: float = 1e-5\n",
        "    adam_weight_decay: int = 0\n",
        "    adam_betas: tuple = (0.9, 0.999)\n",
        "    embd_dim: int = (patch_size ** 2) * num_channels           # 768\n",
        "    num_patches: int = (image_size // patch_size) ** 2         # 196\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3AmdVOz13gj"
      },
      "outputs": [],
      "source": [
        "config = vit_config\n",
        "\n",
        "random.seed(config.random_seed)\n",
        "numpy.random.seed(config.random_seed)\n",
        "torch.manual_seed(config.random_seed)\n",
        "torch.cuda.manual_seed(config.random_seed)\n",
        "torch.cuda.manual_seed_all(config.random_seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ek6QL9lR1fq"
      },
      "outputs": [],
      "source": [
        "##################\n",
        "# MODEL BUILDING #\n",
        "##################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5NUJ8bJ16XT"
      },
      "outputs": [],
      "source": [
        "class VisionEmbedding(nn.Module):\n",
        "    def __init__(self, config: vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.config  = config\n",
        "        self.patch_embedding = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=config.num_channels,\n",
        "                out_channels=config.embd_dim,\n",
        "                kernel_size=config.patch_size,\n",
        "                stride=config.patch_size,\n",
        "                padding=\"valid\"\n",
        "            ),\n",
        "            nn.Flatten(start_dim=2)\n",
        "        )\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.randn(size=(1, 1, config.embd_dim)), requires_grad=True)\n",
        "        self.pos_embeddings = nn.Parameter(torch.randn(size=(1, config.num_patches + 1, config.embd_dim)), requires_grad=True)\n",
        "        self.dropout = nn.Dropout(p=config.dropout)\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "\n",
        "        patch_embd = self.patch_embedding(x).transpose(2,1)\n",
        "        patch_embd = torch.cat([cls_token, patch_embd], dim=1)\n",
        "        embd = self.pos_embeddings + patch_embd\n",
        "        embd = self.dropout(embd)\n",
        "        return embd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VVA7KtA19T1"
      },
      "outputs": [],
      "source": [
        "class VisionAttention(nn.Module):\n",
        "    def __init__(self, config:vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_dim = config.embd_dim\n",
        "        self.num_heads = config.num_heads\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        self.q_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "        self.k_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "        self.v_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "        self.out_proj = nn.Linear(self.embd_dim, self.embd_dim)\n",
        "\n",
        "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        query = self.q_proj(x)\n",
        "        key = self.k_proj(x)\n",
        "        value = self.v_proj(x)\n",
        "\n",
        "        query = query.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
        "        key = key.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
        "        value = value.view(B, T, self.num_heads, C//self.num_heads).transpose(1,2)\n",
        "\n",
        "        attn_score = (query @ key.transpose(-2, -1)) * (1.0 / math.sqrt(key.size(-1)))\n",
        "        attn_score = F.softmax(attn_score, dim=-1).to(query.dtype)\n",
        "\n",
        "        attn_out = (attn_score @ value).transpose(1,2)\n",
        "        attn_out = attn_out.reshape(B, T, C).contiguous()\n",
        "        attn_out = self.out_proj(attn_out)\n",
        "        attn_out = F.dropout(attn_out, p=self.dropout, training=self.training)\n",
        "\n",
        "        return attn_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Md6GkSB58bZC"
      },
      "outputs": [],
      "source": [
        "class VisionMLP(nn.Module):\n",
        "    def __init__(self, config:vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(config.embd_dim, 3 * config.embd_dim)\n",
        "        self.layer2 = nn.Linear(3 * config.embd_dim, config.embd_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.layer1(x)\n",
        "        x = nn.functional.gelu(x, approximate=\"tanh\")\n",
        "        x = self.layer2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj7oNdsh9I0A"
      },
      "outputs": [],
      "source": [
        "class VisionEncoderLayer(nn.Module):\n",
        "    def __init__(self, config: vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embd_dim = config.embd_dim\n",
        "        self.attn = VisionAttention(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(self.embd_dim, eps=config.layer_norm_eps)\n",
        "        self.mlp = VisionMLP(config)\n",
        "        self.layer_norm2 = nn.LayerNorm(self.embd_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attn(self.layer_norm1(x))\n",
        "        x = x + self.mlp(self.layer_norm2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zXgCqcM9V_A"
      },
      "outputs": [],
      "source": [
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, config: vit_config):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([VisionEncoderLayer(config) for _ in range(config.num_encoder_layers)])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwMHqcej9Ziw"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config=vit_config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = VisionEmbedding(config)\n",
        "        self.encoder = VisionEncoder(config)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(config.embd_dim, eps=config.layer_norm_eps),\n",
        "            nn.Linear(config.embd_dim, config.num_classes)\n",
        "        )\n",
        "\n",
        "        self.bbox_head = nn.Sequential(\n",
        "            nn.LayerNorm(config.embd_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.embd_dim, 4)  # [x_min, y_min, x_max, y_max]\n",
        "        )\n",
        "\n",
        "    def forward(self, x:torch.Tensor ) -> torch.Tensor:\n",
        "        x = self.embedding(x)\n",
        "        x = self.encoder(x)\n",
        "        logits = self.mlp_head(x[:, 0, :])\n",
        "        bbox = torch.sigmoid(self.bbox_head(x[:, 0, :]))\n",
        "\n",
        "        return logits, bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWRczUIu9lxY"
      },
      "outputs": [],
      "source": [
        "vit = VisionTransformer(vit_config)\n",
        "summary(model=vit,\n",
        "        input_size=(16, 3, 224, 224),\n",
        "        col_names= [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings= [\"var_names\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJsMeb76_fc3"
      },
      "outputs": [],
      "source": [
        "config = vit_config\n",
        "model = VisionTransformer(config)\n",
        "model.to(config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbeDufRuTG8A"
      },
      "outputs": [],
      "source": [
        "##################\n",
        "# MODEL TRAINING #\n",
        "##################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORF_tSPTAO7-"
      },
      "outputs": [],
      "source": [
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "criterion_bbox = nn.SmoothL1Loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, betas=config.adam_betas, weight_decay=config.adam_weight_decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config.epochs)\n",
        "\n",
        "for epoch in range(config.epochs):\n",
        "    model.train()\n",
        "    train_loss_running = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        img = batch[\"image\"].float().to(config.device)\n",
        "        label = batch[\"label\"].long().to(config.device)\n",
        "        bbox  = batch[\"bbox\"].float().to(config.device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, bbox_pred = model(img)\n",
        "\n",
        "        loss_cls = criterion_cls(logits, label)\n",
        "        loss_bbox = criterion_bbox(bbox_pred, bbox)\n",
        "        loss = loss_cls + loss_bbox  # you can weight: loss = loss_cls + 5*loss_bbox\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_running += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss_running / len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk2yTRA-TRIC"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# MODEL EVALUATION #\n",
        "####################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_prediction(img_tensor, bbox_pred):\n",
        "    img_np = img_tensor.permute(1,2,0).cpu().numpy()\n",
        "    h, w = img_np.shape[:2]\n",
        "\n",
        "    xmin, ymin, xmax, ymax = bbox_pred\n",
        "    xmin *= w\n",
        "    xmax *= w\n",
        "    ymin *= h\n",
        "    ymax *= h\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import matplotlib.patches as patches\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(img_np)\n",
        "    rect = patches.Rectangle(\n",
        "        (xmin, ymin),\n",
        "        xmax - xmin,\n",
        "        ymax - ymin,\n",
        "        fill=False,\n",
        "        edgecolor='red',\n",
        "        linewidth=2\n",
        "    )\n",
        "    ax.add_patch(rect)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HW52ptMQfFfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "batch = next(iter(loader))\n",
        "img = batch[\"image\"][0].unsqueeze(0).to(config.device)\n",
        "_, bbox = model(img)\n",
        "show_prediction(batch[\"image\"][0], bbox[0].detach().cpu())"
      ],
      "metadata": {
        "id": "FybaIScNfGo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DY2_I0RFm5VU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPIMjWnGKFGMmI71WR49PUn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}