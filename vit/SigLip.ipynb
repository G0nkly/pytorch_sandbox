{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEyP632+P23lz61yVRE4uZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vit/SigLip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixdzGoEKFZav"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionConfig:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_size=768,\n",
        "      intermediate_size=3072,\n",
        "      num_hidden_layers=12,\n",
        "      num_attention_heads=12,\n",
        "      num_channels=3,\n",
        "      image_size=224,\n",
        "      patch_size=16,\n",
        "      layer_norm_eps=1e-6,\n",
        "      attention_dropout=0.0,\n",
        "      num_image_tokens: int = None,\n",
        "      **kwargs\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.num_channels = num_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.image_size = image_size\n",
        "    self.attention_dropout = attention_dropout\n",
        "    self.layer_norm_eps = layer_norm_eps\n",
        "    self.num_image_tokens = num_image_tokens"
      ],
      "metadata": {
        "id": "FeA9FcIeFg1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.embed_dim = config.hidden_size\n",
        "    self.image_size = config.image_size\n",
        "    self.patch_size = config.patch_size\n",
        "\n",
        "    self.patch_embedding = nn.Conv2d(\n",
        "        in_channels=config.num_channels,\n",
        "        out_channels=self.embed_dim,\n",
        "        kernel_size=self.patch_size,\n",
        "        stride=self.patch_size,\n",
        "        padding=\"valid\" # this indicates that no padding is added\n",
        "    )\n",
        "\n",
        "    self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "    self.num_positions = self.num_patches\n",
        "    self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
        "    self.register_buffer(\n",
        "        \"position_ids\",\n",
        "        torch.arange(self.num_positions).expand((1, -1)),\n",
        "        persistent=False\n",
        "    )\n",
        "\n",
        "  def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
        "    _, _, height, width = pixel_values.shape # (B, C, H, W)\n",
        "    patch_embeds = self.patch_embedding(pixel_values) # (B, embed_dim, num_patches[num_patches_h * num_patches_w])\n",
        "    embeddings = patch_embeds.flatten(2)\n",
        "    embeddings = embeddings.transpose(1, 2) # (B, num_patches, embed_dim)\n",
        "    embeddings = embeddings + self.position_embedding(self.position_ids)\n",
        "    return embeddings # (B, num_patches, embed_dim)\n"
      ],
      "metadata": {
        "id": "80NXAriP1xPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.embed_dim = config.hidden_size\n",
        "    self.num_heads = config.num_attention_heads\n",
        "    self.head_dim = self.embed_dim // self.num_heads\n",
        "    self.scale = self.head_dim**-0.5 # equivalent to 1 / sqrt(self.head_dim)\n",
        "    self.dropout = config.attention_dropout\n",
        "\n",
        "    self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "    self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      hidden_states: torch.Tensor,\n",
        "  ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
        "    batch_size, seq_len, _ = hidden_states.size()   # (B, num_patches, embed_dim)\n",
        "    query_states = self.q_proj(hidden_states)       # (B, num_patches, embed_dim)\n",
        "    key_states = self.k_proj(hidden_states)         # (B, num_patches, embed_dim)\n",
        "    values_states = self.v_proj(hidden_states)      # (B, num_patches, embed_dim)\n",
        "    query_states = query_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2) # (B, num_heads, num_patches, head_dim)\n",
        "    key_states = key_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1,2)     # (B, num_heads, num_patches, head_dim)\n",
        "    value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim)                # (B, num_heads, num_patches, head_dim)\n",
        "    # attention formula: Q * K^T / sqrt(d_k)\n",
        "    attn_weights = (torch.matmul(query_states, key_states.transpose(2,3)) * self.scale)                 # (B, num_heads, num_patches, num_patches)\n",
        "\n",
        "    if attn_weights.size() != (batch_size, self.num_heads, seq_len, seq_len):\n",
        "      raise ValueError(\n",
        "          f\"Attention weights should be of size {(batch_size, self.num_heads, seq_len, seq_len)}, but is\"\n",
        "          f\" {attn_weights.size()}\"\n",
        "      )\n",
        "\n",
        "    # Apply softmax row-wise. attn_weights (B, num_heads, num_patches, num_patches)\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "    # Apply dropout only during training\n",
        "    attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "    attn_output = torch.matmul(attn_weights, values_states)\n",
        "\n",
        "    if attn_output.size() != (batch_size, self.num_heads, seq_len, self.head_dim):\n",
        "      raise ValueError(\n",
        "          f\"attn_output should be of size {(batch_size, self.num_heads, seq_len, self.head_dim)}, but is\"\n",
        "          f\"{attn_output.size()}\"\n",
        "      )\n",
        "\n",
        "      attn_output = attn_output.transpose(1,2).contiguous() # (B, num_patches, num_heads, head_dim)\n",
        "      attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n"
      ],
      "metadata": {
        "id": "w4t0BnAvuF67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipMLP(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "    self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "\n",
        "  def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "    hidden_states = self.fc1(hidden_states)\n",
        "    hidden_states = nn.functional.gelu(hidden_states, approximate=\"tanh\")\n",
        "    hidden_states = self.fc2(hidden_states)\n",
        "    return hidden_states"
      ],
      "metadata": {
        "id": "PDoUeNaQqv_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipEncoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.embed_dim = config.hidden_size\n",
        "    self.self_attn = SiglipAttention(config)\n",
        "    self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
        "    self.mlp = SiglipMLP(config)\n",
        "    self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "    residual = hidden_states # (B, num_patches, embed_dim)\n",
        "    hidden_states = self.layer_norm1(hidden_states) # (B, num_patches, embed_dim)\n",
        "    hidden_states, _ = self.self_attn(hidden_states=hidden_states) # (B, num_patches, embed_dim)\n",
        "    hidden_states = residual + hidden_states # (B, num_patches, embed_dim)\n",
        "    residual = hidden_states\n",
        "    hidden_states = self.layer_norm2(hidden_states) # (B, num_patches, embed_dim)\n",
        "    hidden_states = self.mlp(hidden_states) # (B, num_patches, embed_dim)\n",
        "    hidden_states = residual + hidden_states # (B, num_patches, embed_dim)\n",
        "\n",
        "    return hidden_states\n"
      ],
      "metadata": {
        "id": "S71Vyr76aWQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionTransformer(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    embed_dim = config.hidden_size\n",
        "    self.embeddings = SiglipVisionEmbeddings(config)\n",
        "    self.encoder = SiglipEncoder(config)\n",
        "    self.post_layernorm = nn.LayerNorm(embedd_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "    # B, C, H, W -> B, num_patches, embed_dim\n",
        "    hidden_states = self.embeddings(pixel_values)\n",
        "    last_hidden_state = self.encoder(input_embed=hidden_states)\n",
        "    last_hidden_state = self.post_layernorm(last_hidden_state)\n",
        "\n",
        "    return last_hidden_state\n",
        "\n",
        "class SiglipVisionModel(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.vision_model = SiglipVisionTransformer(config)\n",
        "\n",
        "  def forward(self, pixel_values) -> Tuple:\n",
        "    # B, C, H, W -> B, num_patches, embed_dim\n",
        "    return self.vision_model(pixel_values=pixel_values)"
      ],
      "metadata": {
        "id": "3R2FDvdvGol9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsHqhot51vs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}