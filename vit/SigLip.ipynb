{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh4vrADTdc034aQ4BZQsY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vit/SigLip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ixdzGoEKFZav"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionConfig:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_size=768,\n",
        "      intermediate_size=3072,\n",
        "      num_hidden_layers=12,\n",
        "      num_attention_heads=12,\n",
        "      num_channels=3,\n",
        "      image_size=224,\n",
        "      patch_size=16,\n",
        "      layer_norm_eps=1e-6,\n",
        "      attention_dropout=0.0,\n",
        "      num_image_tokens: int = None,\n",
        "      **kwargs\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.num_channels = num_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.image_size = image_size\n",
        "    self.attention_dropout = attention_dropout\n",
        "    self.layer_norm_eps = layer_norm_eps\n",
        "    self.num_image_tokens = num_image_tokens"
      ],
      "metadata": {
        "id": "FeA9FcIeFg1Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionTransformer(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    embed_dim = config.hidden_size\n",
        "    self.embeddings = SiglipVisionEmbeddings(config)\n",
        "    self.encoder = SiglipEncoder(config)\n",
        "    self.post_layernorm = nn.LayerNorm(embedd_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "    # B, C, H, W -> B, num_patches, embed_dim\n",
        "    hidden_states = self.embeddings(pixel_values)\n",
        "    last_hidden_state = self.encoder(input_embed=hidden_states)\n",
        "    last_hidden_state = self.post_layernorm(last_hidden_state)\n",
        "\n",
        "    return last_hidden_state\n",
        "\n",
        "class SiglipVisionModel(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.vision_model = SiglipVisionTransformer(config)\n",
        "\n",
        "  def forward(self, pixel_values) -> Tuple:\n",
        "    # B, C, H, W -> B, num_patches, embed_dim\n",
        "    return self.vision_model(pixel_values=pixel_values)"
      ],
      "metadata": {
        "id": "3R2FDvdvGol9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}