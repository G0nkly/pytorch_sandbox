{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4AbOw7574gKn5YfwEOxeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vit/SigLip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixdzGoEKFZav"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionConfig:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      hidden_size=768,\n",
        "      intermediate_size=3072,\n",
        "      num_hidden_layers=12,\n",
        "      num_attention_heads=12,\n",
        "      num_channels=3,\n",
        "      image_size=224,\n",
        "      patch_size=16,\n",
        "      layer_norm_eps=1e-6,\n",
        "      attention_dropout=0.0,\n",
        "      num_image_tokens: int = None,\n",
        "      **kwargs\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.num_channels = num_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.image_size = image_size\n",
        "    self.attention_dropout = attention_dropout\n",
        "    self.layer_norm_eps = layer_norm_eps\n",
        "    self.num_image_tokens = num_image_tokens"
      ],
      "metadata": {
        "id": "FeA9FcIeFg1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.embed_dim = config.hidden_size\n",
        "    self.image_size = config.image_size\n",
        "    self.patch_size = config.patch_size\n",
        "\n",
        "    self.patch_embedding = nn.Conv2d(\n",
        "        in_channels=config.num_channels,\n",
        "        out_channels=self.embed_dim,\n",
        "        kernel_size=self.patch_size,\n",
        "        stride=self.patch_size,\n",
        "        padding=\"valid\" # this indicates that no padding is added\n",
        "    )\n",
        "\n",
        "    self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "    self.num_positions = self.num_patches\n",
        "    self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n",
        "    self.register_buffer(\n",
        "        \"position_ids\",\n",
        "        torch.arange(self.num_positions).expand((1, -1))\n",
        "        persistent=False\n",
        "    )\n",
        "\n",
        "  def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n",
        "    _, _, height, width = pixel_values.shape # (B, C, H, W)\n",
        "    patch_embeds = self.patch_embedding(pixel_values) # (B, embed_dim, num_patches[num_patches_h * num_patches_w])\n",
        "    embeddings = patch_embeds.flatten(2)\n",
        "    embeddings = embeddings.transpose(1, 2) # (B, num_patches, embed_dim)\n",
        "    embeddings = embeddings + self.position_embedding(self.position_ids)\n",
        "    return embeddings # (B, num_patches, embed_dim)\n"
      ],
      "metadata": {
        "id": "80NXAriP1xPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SiglipVisionTransformer(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    embed_dim = config.hidden_size\n",
        "    self.embeddings = SiglipVisionEmbeddings(config)\n",
        "    self.encoder = SiglipEncoder(config)\n",
        "    self.post_layernorm = nn.LayerNorm(embedd_dim, eps=config.layer_norm_eps)\n",
        "\n",
        "  def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "    # B, C, H, W -> B, num_patches, embed_dim\n",
        "    hidden_states = self.embeddings(pixel_values)\n",
        "    last_hidden_state = self.encoder(input_embed=hidden_states)\n",
        "    last_hidden_state = self.post_layernorm(last_hidden_state)\n",
        "\n",
        "    return last_hidden_state\n",
        "\n",
        "class SiglipVisionModel(nn.Module):\n",
        "\n",
        "  def __init__(self, config: SiglipVisionConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.vision_model = SiglipVisionTransformer(config)\n",
        "\n",
        "  def forward(self, pixel_values) -> Tuple:\n",
        "    # B, C, H, W -> B, num_patches, embed_dim\n",
        "    return self.vision_model(pixel_values=pixel_values)"
      ],
      "metadata": {
        "id": "3R2FDvdvGol9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "ab6b64a3-a004-43d3-91c2-414508637099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Tuple' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-433469032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSiglipVisionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSiglipVisionConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3-433469032.py\u001b[0m in \u001b[0;36mSiglipVisionModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiglipVisionTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;31m# B, C, H, W -> B, num_patches, embed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LsHqhot51vs9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}