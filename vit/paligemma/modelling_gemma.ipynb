{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16rYHM5V6eGCg6iR56bNvmzHh3ykw6iJI",
      "authorship_tag": "ABX9TyNFbJq/XslvUfDSnbUnqZuV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vit/paligemma/modelling_gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lHDvljyvLOiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install import-ipynb"
      ],
      "metadata": {
        "id": "G0kWqCa9Mckb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive/Colab\\ Notebooks/PyTorch\\ Transformer/Vision\\ Transformer/paligemma"
      ],
      "metadata": {
        "id": "U-Gsl8qkNul2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"drive/MyDrive/Colab\\ Notebooks/PyTorch\\ Transformer/Vision\\ Transformer/paligemma\")\n",
        "os.cwd(\"drive/MyDrive/Colab\\ Notebooks/PyTorch\\ Transformer/Vision\\ Transformer/paligemma\")"
      ],
      "metadata": {
        "id": "zxdsVs4CMt22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ-vfdzZ5EyK"
      },
      "outputs": [],
      "source": [
        "import import_ipynb\n",
        "from siglip import SiglipVisionConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional, Tuple, List\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import math"
      ],
      "metadata": {
        "id": "jH1JNkBVghEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaConfig:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      hidden_size,\n",
        "      intermediate_size,\n",
        "      num_hidden_layers,\n",
        "      num_attention_heads,\n",
        "      num_key_value_heads,\n",
        "      head_dim=256,\n",
        "      max_position_embedding=8192,\n",
        "      rms_norm_eps=1e-6,\n",
        "      rope_theta=10000.0,\n",
        "      attention_bias=False,\n",
        "      attention_dropout=0.0,\n",
        "      pad_token_id=None,\n",
        "      **kwargs\n",
        "  ):\n",
        "\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.max_position_embedding = max_position_embedding\n",
        "    self.hidden_size = hidden_size\n",
        "    self.intermediate_size = intermediate_size\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.num_attention_heads = num_attention_heads\n",
        "    self.head_dim = head_dim\n",
        "    self.num_key_value_heads = num_key_value_heads\n",
        "    self.rms_norm_eps = rms_norm_eps\n",
        "    self.rope_theta = rope_theta\n",
        "    self.attention_bias = attention_bias\n",
        "    self.attention_dropout = attention_dropout\n",
        "    self.pad_token_id = pad_token_id"
      ],
      "metadata": {
        "id": "XW4VmLXskGlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaliGemmaConfig:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      vision_config=None,\n",
        "      text_config=None,\n",
        "      ignore_index=-100,\n",
        "      image_token_index=256000,\n",
        "      vocab_size=257152,\n",
        "      projection_dim=2048,\n",
        "      hidden_size=2048,\n",
        "      pad_token_id=None,\n",
        "      **kwargs\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.ignore_index = ignore_index\n",
        "    self.image_token_index = image_token_index\n",
        "    self.vocab_size = vocab_size\n",
        "    self.projection_dim = projection_dim\n",
        "    self.hidden_size = hidden_size\n",
        "    self.vision_config = vision_config\n",
        "    self.is_encoder_decoder = False,\n",
        "    self.pad_token_id = pad_token_id\n",
        "\n",
        "    self.vision_config = SiglipVisionConfig(**vision_config)\n",
        "    self.text_config = text_config\n",
        "    self.text_config = GemmaConfig(**text_config, pad_token_id=pad_token_id)\n",
        "    self.vocab_size = self.text_config.vocab_size\n",
        "    self.text_config.num_image_tokens = (self.vision_config.image_size // self.vision_config.patch_size) ** 2\n",
        "    self.vision_config.projection_dim = projection_dim"
      ],
      "metadata": {
        "id": "Mbx1H2xzgZWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaRMSNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, dim: int, eps: float = 1e-6):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.weight = nn.Parammeter(torch.zeros(dim))\n",
        "\n",
        "  def norm(self, x):\n",
        "    return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self._norm(x.float())\n",
        "    # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)\n",
        "    # See https://github.com/huggingface/transformers/pull/29402\n",
        "    output = output * (1.0 + self.weight.float())\n",
        "\n",
        "    return output.type_as(x)"
      ],
      "metadata": {
        "id": "ByTfFAykro7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KVCache():\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    self.key_cache: List[torch.Tenor] = []\n",
        "    self.value_cache: List[torch.Tensor] = []\n",
        "\n",
        "  def num_items(self) -> int:\n",
        "    if len(self.key_cache) == 0:\n",
        "      return 0\n",
        "\n",
        "    else:\n",
        "      # The shape of the key_cache is (B, num_heads_kv, seq_len, head_dim)\n",
        "      return self.key_cache[0].shape[-2]\n",
        "\n",
        "  def update(\n",
        "      self,\n",
        "      key_states: torch.Tensor,\n",
        "      value_states: torch.Tensor,\n",
        "      layer_idx: int\n",
        "  ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    if len(self.key_cache) <= layer_idx:\n",
        "      # If we never added anything to the KV_Cache of this layer, let's create it\n",
        "      self.key_cache.append(key_states)\n",
        "      self.value_cache.append(value_states)\n",
        "    else:\n",
        "      # ... otherwise we concatenate the new keys with the existing ones.\n",
        "      # each tensor has shape: (B, num_heads_kv, seq_len, head_dim)\n",
        "      self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n",
        "      self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n",
        "\n",
        "    # ... and then we return all the existing keys + the new ones.\n",
        "    return self.key_cache[layer_idx], self.value_cache[layer_idx]"
      ],
      "metadata": {
        "id": "QvY_J7J7_KHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaMLP(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.hidden_size = config.hidden_size\n",
        "    self.intermediate_size = config.intermediate_size\n",
        "    self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "    self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
        "    self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.down_proj(nn.functional.gelu(self.gate_proj(x), approximate=\"tanh\") * self.up_proj(x))"
      ],
      "metadata": {
        "id": "lgCb_nrSRrfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "  batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
        "  if n_rep == 1:\n",
        "    return hidden_states\n",
        "\n",
        "  hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
        "\n",
        "  return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
      ],
      "metadata": {
        "id": "pJdF0j0bRuIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaRotaryEmbedding(nn.Module):\n",
        "  def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dim = dim\n",
        "    self.max_position_embeddings = max_position_embeddings\n",
        "    self.base = base\n",
        "\n",
        "    # Calculate the theta according to the formula theta_i = base^(2i/dim) where i=0, 1, 2, ...,dim // 2\n",
        "    inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n",
        "    self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def forward(self, x, position_ids, seq_len=None):\n",
        "    # x (B, num_attention_heads, seq_len, head_size)\n",
        "    self.inv_freq.to(x.device)\n",
        "    inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1) # (B, head_dim // 2, 1)\n",
        "    position_ids_expanded = position_ids[:, None, :].float()\n",
        "    device_type = x.device_type\n",
        "    device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
        "    with torch.autocast(device_type=device_type, enabled=False):\n",
        "      freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1,2) # (B, head_dim // 2, 1) @ (B, 1, seq_len) -> (B, seq_len, head_dim // 2)\n",
        "      emb = torch.cat((freqs, freqs), dim=-1)\n",
        "      cos = emb.cos()   # (B, seq_len, head_dim)\n",
        "      sind = emb.sin()  # (B, seq_len, head_dim)\n",
        "\n",
        "    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xve8VBiwPGWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.layer_idx = layer_idx\n",
        "    self.attention_dropout = config.attention_dropout\n",
        "    self.hidden_size = config.hidden_size\n",
        "    self.num_heads = config.num_attention_heads\n",
        "    self.head_dim = config.head_dim\n",
        "    self.num_key_value_heads = config.num_key_value_heads\n",
        "    self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
        "    self.max_position_embeddings = config.max_position_embeddings\n",
        "    self.rope_theta = config.rope_theta\n",
        "    self.is_causal = True\n",
        "\n",
        "    assert self.hidden_size % self.num_heads == 0\n",
        "\n",
        "    self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
        "    self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "    self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
        "    self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n",
        "\n",
        "    self.rotary_emb = GemmaRotaryEmbedding(\n",
        "        self.head_dim,\n",
        "        max_position_embedding=self.max_position_embeddings,\n",
        "        base=self.rope_theta\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      hidden_states: torch.Tensor,\n",
        "      attention_mask: Optional[torch.Tensor] = None,\n",
        "      position_ids: Optional[torch.LongTensor] = None,\n",
        "      kv_cache: Optional[KVCache] = None,\n",
        "      **kwargs\n",
        "  ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "    bsz, q_len, _ = hidden_states.size() # (B, seq_len, hidden_size)\n",
        "    query_states = self.q_proj(hidden_states) # (B, seq_len, num_heads_q * head_dim)\n",
        "    key_states = self.k_proj(hidden_states) # (B, seq_len, num_heads_kv * head_dim)\n",
        "    values_states = self.v_proj(hidden_states) # (B, seq_len, num_heads_kv * head_dim)\n",
        "    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2) # (B, num_heads_q, seq_len, head_dim)\n",
        "    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, num_heads_kv, seq_len, head_dim)\n",
        "    values_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2) # (B, num_heads_kv, seq_len, head_dim)\n",
        "    cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None) # (B, seq_len, head_dim)\n",
        "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin) # (B, num_heads_q, seq_len, head_dim), (B, num_heads_kv, seq_len, head_dim)\n",
        "\n",
        "    if kv_cache is not None:\n",
        "      key_states, value_states = kv_cache.update(key_states, value_states, self.layer_idx)\n",
        "\n",
        "    # Repeat the key and values to match the number of heads of the query\n",
        "    key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
        "    value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
        "\n",
        "    # Perform the calculation as usual, Q * K^T / sqrt(head_dim). Shape: (B, num_heads_q, seq_len_q, seq_len_kv)\n",
        "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "\n",
        "    assert attention_mask is not None\n",
        "    attn_weights = attn_weights + attention_mask\n",
        "\n",
        "    # Apply the softmax\n",
        "    # (B, num_heads_q, seq_len_q, seq_len_kv)\n",
        "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "    # Apply the dropout\n",
        "    attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
        "    # Multiply by the values. (B, num_heads_q, seq_len_q, seq_len_kv) x (B, num_heads_kv, seq_len_kv, head_dim) -> (B, num_heads, seq_len_q, head_dim)\n",
        "    attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "    if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
        "      raise ValueError(\n",
        "          f\"'attn_output' should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
        "          f\" {attn_output.size()}\"\n",
        "      )\n",
        "\n",
        "    # Make sure the sequence length is the second dimension # (B, num_heads_q, seq_len_q, head_dim) -> (B, seq_len_q, num_heads_q, head_dim)\n",
        "    attn_output = attn_output.transpose(1,2).contiguous()\n",
        "    # Concatenate all the heads together (B, seq_len_q, num_heads_q, head_dim) -> (B, seq_len_q, num_heads_q * head_dim)\n",
        "    attn_output = attn_output.view(bsz, q_len, -1)\n",
        "\n",
        "    # Multiply by W_o (B, seq_len_q, hidden_size)\n",
        "    attn_output = self.o_proj(attn_output)\n",
        "\n",
        "    return attn_output, attn_weights"
      ],
      "metadata": {
        "id": "CT0IfzxSj5ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaDecoderLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, config: GemmaConfig, layer_idx: int):\n",
        "    super().__init__()\n",
        "    self.hidden_size = config.hidden_size\n",
        "    self.self_attn = GemmaAttention(config=config, layer_idx=layer_idx)\n",
        "    self.mlp = GemmaMLP(config)\n",
        "    self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "    self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      hidden_states: torch.Tensor,\n",
        "      attention_mask: Optional[torch.Tensor] = None,\n",
        "      position_ids: Optional[torch.LongTensor] = None,\n",
        "      kv_cache: Optional[KVCache] = None\n",
        "  ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
        "    residual = hidden_states\n",
        "    hidden_states = self.input_layernorm(hidden_states) # (B, seq_len, hidden_size)\n",
        "\n",
        "    hidden_states, _ = self.self_attn(\n",
        "        hidden_states=hidden_states,\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        kv_cache=kv_cache\n",
        "    )\n",
        "\n",
        "    hidden_states = residual + hidden_states # (B, seq_len, hidden_size)\n",
        "\n",
        "    residual = hidden_states\n",
        "\n",
        "    hidden_states = self.post_attention_layernorm(hidden_states) # (B, seq_len, hidden_size)\n",
        "\n",
        "    hidden_states = self.mlp(hidden_states)\n",
        "\n",
        "    hidden_states = residual + hidden_states\n",
        "\n",
        "    return hidden_states\n"
      ],
      "metadata": {
        "id": "YdnK1jKotHgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaModel(nn.Module):\n",
        "\n",
        "  def __init__(self, config: GemmaConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.padding_idx = config.pad_token_id\n",
        "    self.vocab_size = config.vocab_size\n",
        "\n",
        "    self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
        "    )\n",
        "    self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
        "\n",
        "  def get_input_embeddings(self):\n",
        "    return self.embed_tokens\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      attention_mask: Optional[torch.Tensor] = None,\n",
        "      position_ids: Optional[torch.LongTensor] = None,\n",
        "      inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "      kv_cache: Optional[KVCache] = None\n",
        "  ) -> torch.FloatTensor:\n",
        "    hidden_states = inputs_embeds # (B, seq_len, hidden_size)\n",
        "    normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n",
        "    hidden_states = hidden_states * normalizer\n",
        "\n",
        "    for decoder_layer in self.layers:\n",
        "      hidden_states = decoder_layer( # (B, seq_len, hidden_size)\n",
        "          hidden_states,\n",
        "          attention_mask=attention_mask,\n",
        "          position_ids=position_ids,\n",
        "          kv_cache=kv_cache\n",
        "      )\n",
        "\n",
        "    hidden_states = self.norm(hidden_states) # (B, seq_len, hidden_size)\n",
        "\n",
        "    return hidden_states"
      ],
      "metadata": {
        "id": "HGRwY_sV7pe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaForCausalLM(nn.Module):\n",
        "\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.model = GemmaModel(config)\n",
        "    self.vocab_size = config.vocab_size\n",
        "    self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "\n",
        "  def get_input_embeddings(self):\n",
        "    return self.model.embed_tokens\n",
        "\n",
        "  def tie_weights(self):\n",
        "    self.lm_head.weight = self.model.embed_tokens.weight\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      attention_mask: Optional[torch.Tensor] = None,\n",
        "      position_ids: Optional[torch.LongTensor] = None,\n",
        "      inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "      kv_cache: Optional[KVCache] = None\n",
        "  ) -> Tuple:\n",
        "    outputs = self.model(\n",
        "        attention_mask=attention_maks,\n",
        "        position_ids=position_ids,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        kv_cache=kv_cache,\n",
        "    )\n",
        "\n",
        "    hidden_states = outputs\n",
        "    logits = self.lm_head(hidden_states)\n",
        "    logits = logits.float()\n",
        "\n",
        "    return_data = {\n",
        "        \"logits\": logits,\n",
        "    }\n",
        "\n",
        "    if kv_cache is not None:\n",
        "      # Return the updated cache\n",
        "      return_data[\"kv_cache\"] = kv_cache\n",
        "\n",
        "    return kv_cache"
      ],
      "metadata": {
        "id": "YlmnQWTCcxSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaliGemmaMultiModalProjector(nn.Module):\n",
        "\n",
        "  def __init__(self, config: PaliGemmaConfig):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(config.vision_config.hidden_size, config.vision_config.projection_dim, bias=True)\n",
        "\n",
        "  def forward(self, image_features):\n",
        "    hidden_states = self.linear(image_features) # (B, num_patches, projection_dim)\n",
        "    return hidden_states"
      ],
      "metadata": {
        "id": "GOMC68InbrK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PaliGemmaForConditionalGeneration(nn.Module):\n",
        "\n",
        "  def __init__(self, config: PaliGemmaConfig):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "    self.vision_tower = SiglipVisionModel(config.vision_config)\n",
        "    self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n",
        "    self.vocab_size = config.vocab_size\n",
        "\n",
        "    language_model = GemmaForCausalLM(config.text_config)\n",
        "    self.language_model = language_model\n",
        "    self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
        "\n",
        "  def tie_weights(self):\n",
        "    return self.language_model.tie_weights()\n",
        "\n",
        "  def _merge_input_ids_with_image_features(\n",
        "      self, image_features: torch.Tensor,\n",
        "      inputs_embeds: torch.Tensor,\n",
        "      input_ids: torch.Tensor,\n",
        "      attention_mask: torch.Tensor,\n",
        "      kv_cache: KVCache\n",
        "  ):\n",
        "    _, _, embed_dim = image_features.shape\n",
        "    batch_size, sequence_length = input_ids.shape\n",
        "    dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
        "    scaled_image_features = image_features / (self.config.hidden_size**0.5) # (B, seq_len, hidden_size)\n",
        "\n",
        "    # combine the embeddings of the image tokens, the text tokens and mask out all the padding tokens\n",
        "    final_embedding = torch.zeros(batch_size, sequence_length, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n",
        "    text_mask = (input_ids != self.config.image_token_index) & (input_ids != self.pad_token_id) # (B, seq_len)\n",
        "    image_mask = input_ids == self.config.image_token_index # (B, seq_len)\n",
        "    pad_mask = input_ids == self.pad_token_id # (B, seq_len)\n",
        "\n",
        "    # We need to expand the masks to the embedding dimension otherwise we can't use them in torch.where\n",
        "    text_mask_expanded = text_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
        "    pad_mask_expanded = pad_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
        "    image_mask_expanded = image.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
        "\n",
        "    # add the text embedding\n",
        "    final_embedding = torch.where(text_mask_expanded, inputs_embeds, final_embedding)\n",
        "    # insert image embeddings. We can't use torch.where because sequence length of scaled_image_features is different from final_embedding\n",
        "    final_embedding = torch.masked_scatter(image_mask_expanded, scaled_image_features)\n",
        "    # Zero out padding tokens\n",
        "    final_embedding = torch.where(pad_mask_expanded, toch.zeros_like(final_embedding), final_embedding)\n",
        "\n",
        "    ### CREATE THE ATTENTION MASK ###\n",
        "    dtype, device = inputs_embeds.dtype, inputs_embeds.device\n",
        "    min_dtype = torch.finfo(dtype).min\n",
        "    q_len = inputs_embeds.shape[1]\n",
        "\n",
        "    if kv_cache is None or kv_cache.num_items() == 0:\n",
        "      # Do not mask any token, because we're in the prefill phase\n",
        "      # This only works when we have no padding\n",
        "      causal_mask = torch.full(\n",
        "          (batch_size, q_len, q_len), fill_value=0, dtype=dtype, device=device\n",
        "      )\n",
        "    else:\n",
        "      # Since we are generating tokens, the query must be one single token\n",
        "      assert q_len == 1\n",
        "      kv_len = kv_cache.num_items() + q_len\n",
        "      # Also in this case we don't need to mask anything, since each query shoud be able to attend all previous tokens\n",
        "      # this only works when we have no padding\n",
        "      causal_mask = torch.full(\n",
        "          (batch_size, q_len, kv_len), fill_value=0, dtype=dtype, device=device\n",
        "      )\n",
        "\n",
        "      # Add the head dimension\n",
        "      causal_mask = causal_mask.unsqueeze(1) # (B, num_heads_q, q_len, kv_len)\n",
        "\n",
        "      if kv_cache is not None and kv_cache.num_items() > 0:\n",
        "        # the position of the query is just the last position\n",
        "        position_ids = attention_mask.cumsum(-1)[:, -1]\n",
        "        if position_ids.dim() == 1:\n",
        "          position_ids = position_ids.unsqueeze(0)\n",
        "\n",
        "      else:\n",
        "        # Create a position_ids based on the size of the attention_mask\n",
        "        # For masked tokens, use the number 1 as position\n",
        "        position_ids = (attention_mask.cumsum(-1)).masked_fill_((attention_mask == 0), 1).to(device)\n",
        "\n",
        "    return final_embedding, causal_mask, position_ids\n",
        "\n",
        "\n",
        "  def forward(\n",
        "      self,\n",
        "      input_ids: torch.LongTensor = None,\n",
        "      pixel_values: torch.FloatTensor = None,\n",
        "      attention_mask: Optional[torch.Tensor] = None,\n",
        "      kv_cache: Optional[KVCache] = None,\n",
        "  ) -> Tuple:\n",
        "    assert torch.all(attention_mask == 1), \"The input cannot be padded\"\n",
        "    inputs_embeds = self.language_model.get_input_embeddings()(input_ids) # (B, seq_len, hidden_size)\n",
        "    selected_image_feature = self.vision_tower(pixel_values.to(inputs_embeds.dtype)) # (B, num_patches, embed_dim)\n",
        "    image_features = self.multi_modal_projector(selected_image_feature) # (B, num_patches, hidden_size)\n",
        "    # Merge the embeddings of the text tokens and the image tokens\n",
        "    input_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(image_features, inputs_embeds, input_ids, attention_mask, kv_cache)\n",
        "\n",
        "    outputs = self.language_model(\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        kv_cache=kv_cache\n",
        "    )\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "mXx-ONek5v6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "zjCkE853L2-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ifxwJffMHPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "knpkwEX6MLrq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}