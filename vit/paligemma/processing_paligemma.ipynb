{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuVOKpuw4gXulvgJhUpHpO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vit/paligemma/processing_paligemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_gxw1XuK4k1z"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, Optional, Union, Tuple, Iterable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_STANDARD_MEAN = [0.5, 0.5, 0.5]\n",
        "IMAGE_STANDARD_STD = [0.5, 0.5, 0.5]"
      ],
      "metadata": {
        "id": "gCZXzwwB4yMy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md\n",
        "\n",
        "# https://huggingface.co/docs/transformers/model_doc/paligemma"
      ],
      "metadata": {
        "id": "9eh2LmAv76tM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(\n",
        "    images: List[Image.Image],\n",
        "    size: Dict[str, int] = None,\n",
        "    resample: Image.Resampling = None,\n",
        "    rescale_factor: float = None,\n",
        "    image_mean: Optional[Union[float, List[float]]] = None,\n",
        "    image_std: Optional[Union[float, List[float]]] = None\n",
        ") -> List[np.ndarray]:\n",
        "  height, width = size[0], size[1]\n",
        "  images = [\n",
        "      resize(image=image, size=(height, width), resample=resample) for image in images\n",
        "  ]\n",
        "  images = [np.array(image) for image in images]\n",
        "  images = [rescale(image, scale=rescale_factor) for image in images]\n",
        "  images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n",
        "  images = [image.transpose(2,0,1) for image in images]\n",
        "  return images\n",
        "\n",
        "class PaliGemmaProcessor:\n",
        "\n",
        "  def __init__(self, tokenizer, num_image_tokens: int, image_size: int):\n",
        "    super().__init__()\n",
        "    self.image_seq_length = num_image_tokens\n",
        "    self.image_size = image_size\n",
        "\n",
        "    tokens_to_add = {\"additional_special_tokens\": [self.IMAGE_TOKEN]}\n",
        "    EXTRA_TOKENS = [\n",
        "        f\"<loc{i:04d}>\" for i in range(1024)\n",
        "    ] # these tokens are used for object detection (bounding boxes)\n",
        "    EXTRA_TOKENS += [\n",
        "        f\"<seq{i:03d}>\" for i in range(128)\n",
        "    ] # these tokens are used for object segmentation\n",
        "    tokenizer.add_tokens(EXTRA_TOKENS)\n",
        "    self.image_token_id = tokenizer.convert_tokens_to_ids(self.IMAGE_TOKENS)\n",
        "    # We will add the BOS and EOS tokens ourselves\n",
        "    tokenizer.add_bos_token = False\n",
        "    tokenizer.add_eos_token = False\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __call__(\n",
        "      self,\n",
        "      text: List[str],\n",
        "      images: List[Image.Image],\n",
        "      padding: str = \"longest\",\n",
        "      truncation: bool = True\n",
        "  ) -> dict:\n",
        "    assert len(images) == 1 and len(text) == 1, f\"Received {len(images)} images for {len(text)} prompts.\"\n",
        "\n",
        "    pixel_values = process_images(\n",
        "        images,\n",
        "        size=(self.image_size, self.image_size),\n",
        "        resample=Image.Resampling.BICUBIC,\n",
        "        rescale_factor=1/255.0,\n",
        "        image_mean=IMAGENET_STANDARD_MEAN,\n",
        "        image_std=IMAGENET_STANDARD_STD,\n",
        "    )\n",
        "\n",
        "    pixel_values = np.stack(pixel_values, axis=0) # (B, channel, height, width)\n",
        "    pixel_values = torch.tensor(pixel_values)\n",
        "\n",
        "    input_strings = [\n",
        "        add_image_tokens_to_prompt(\n",
        "            prefix_prompt=prompt,\n",
        "            bos_token=self.tokenizer.bos_token,\n",
        "            image_seq_len=self.image_seq_length,\n",
        "            image_token=self.IMAGE_TOKEN\n",
        "        )\n",
        "        for prompt in text\n",
        "    ]\n",
        "\n",
        "    inputs = self.tokenizer(\n",
        "        input_strings,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=padding,\n",
        "        truncation=truncation\n",
        "    )\n",
        "\n",
        "    return_data = {\"pixel_values\": pixel_values, **inputs}\n",
        "\n",
        "    return return_data"
      ],
      "metadata": {
        "id": "YeKZoLe748hb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}