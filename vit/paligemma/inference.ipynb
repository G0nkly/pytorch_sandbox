{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM5ceKX1dH53Dlf3hsRnDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/vit/paligemma/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVXUK9IKR8Qd",
        "outputId": "a011b043-1494-4082-86e1-f4ea0bd3d6e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m81.9/87.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (3.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=7845b9436cc46ddf97b4863660ad45a2a4c947a497eba5d5cd99cac2d11910c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XQ5VFHt-Rwdy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "9daecefb-c5c6-4927-eda4-0ed41bb5c73e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'processing_paligemma'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2846165652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprocessing_paligemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPaliGemmaProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodelling_gemma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKVCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPaliGemmaForConditionalGeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_hf_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'processing_paligemma'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import fire\n",
        "from processing_paligemma import PaliGemmaProcessor\n",
        "from modelling_gemma import KVCache, PaliGemmaForConditionalGeneration\n",
        "from utils import load_hf_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def move_inputs_to_device(model_inputs: dict, device: str):\n",
        "  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "gn_kqIBDY68s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _sample_top_p(probs: torch.Tensor, p: float):\n",
        "  probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True) # (B, vocab_size)\n",
        "  probs_sum = torch.cumsum(probs_sort, dim=-1) # (B, vocab_size)\n",
        "  # Subtracting probs_sort shifts the cumulative sum by 1 position to the right before masking\n",
        "  mask = probs_sum - probs_sort > p\n",
        "  # Zero out all the probabilities of tokens that are not selected by the top p\n",
        "  probs_sort[mask] = 0.0\n",
        "  # Redistribute the probabilities so that they sum up to 1\n",
        "  probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "  # Sample a token (its index) from the top p distribution\n",
        "  next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "  # Get the token position in the vocabulary corresponding to the sampled index\n",
        "  next_token = torch.gather(probs_idx, -1, next_token)\n",
        "  return next_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ifejCuDqfjnH",
        "outputId": "39309532-e7c9-4e41-e128-b496376af8c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1980760224.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_sample_top_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprobs_sort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprobs_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_sort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# Subtracting probs_sort shifts the cumulative sum by 1 position to the right before masking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs_sum\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprobs_sort\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_inputs(\n",
        "    processor: PaliGemmaProcessor,\n",
        "    prompt: str,\n",
        "    image_file_path: str,\n",
        "    device: str\n",
        "):\n",
        "  image = Image.open(image_file_path)\n",
        "  images = [image]\n",
        "  prompts = [prompts]\n",
        "  model_inputs = processor(text=prompts, images=images)\n",
        "  model_inputs = move_inputs_to_device(model_inputs, device)\n",
        "\n",
        "  return model_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Mo__9vd1YQZa",
        "outputId": "97b329ab-a9b2-49c0-9a8a-504f1abe1043"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PaliGemmaProcessor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1301249365.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m def get_model_inputs(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprocessor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPaliGemmaProcessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage_file_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PaliGemmaProcessor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(\n",
        "    model: PaliGemmaForConditionalGeneration,\n",
        "    processor: PaliGemmaProcessor,\n",
        "    device: str,\n",
        "    prompt: str,\n",
        "    image_file_path: str,\n",
        "    max_tokens_to_generate: int,\n",
        "    temperature: float,\n",
        "    top_p: float,\n",
        "    do_sample: bool\n",
        "):\n",
        "  model_inputs = get_model_inputs(processor, prompt, image_file_path, device)\n",
        "  input_ids = model_inputs[\"input_ids\"]\n",
        "  attention_mask = model_inputs[\"attention_mask\"]\n",
        "  pixel_values = model_inputs[\"pixel_values\"]\n",
        "\n",
        "  kv_cache = KVCache()\n",
        "\n",
        "  # Generate tokens until you see the stop token\n",
        "  stop_token = processor.tokenizer.eos_token_id\n",
        "  generated_tokens = []\n",
        "\n",
        "  for _ in range(max_tokens_to_generate):\n",
        "    # Get the model outputs\n",
        "    # TODO: remove the labels\n",
        "    outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        pixel_values=pixel_values,\n",
        "        attention_mask=attention_mask,\n",
        "        kv_cache=kv_cache\n",
        "    )\n",
        "    kv_cache = outputs[\"kv_cache\"]\n",
        "    next_token_logits = outputs[\"logits\"][:, -1, :]\n",
        "    # Sample the next token\n",
        "    if do_sample:\n",
        "      # Apply temperature\n",
        "      next_token_logits = torch.softmax(next_token_logits / temperature, dim=-1)\n",
        "      next_token = _sample_top_p(next_token_logits, top_p)\n",
        "    else:\n",
        "      next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "    assert next_token.size() == (1,1)\n",
        "    next_token = next_token.squeeze(0) # Remove batch dimension\n",
        "    generated_tokens.append(next_token)\n",
        "    # Stop if the stop token has been generated\n",
        "    if next_token.item() == stop_token:\n",
        "      break\n",
        "\n",
        "    # Append the next token to the input\n",
        "    input_ids = next_token.unsequeeze(-1)\n",
        "    attention_mask = torch.cat(\n",
        "        [attention_mask, torch.ones((1, 1), device=input_ids.device)], dim=-1\n",
        "    )\n",
        "\n",
        "    # decode the generated tokens\n",
        "    decoded = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    print(prompt + decoded)"
      ],
      "metadata": {
        "id": "5-bgTUmlXGsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main(\n",
        "    model_path: str = None,\n",
        "    prompt:str = None,\n",
        "    image_file_path: str = None,\n",
        "    max_tokens_to_generate: int = 100,\n",
        "    temperature: float = 0.8,\n",
        "    top_p: float = 0.9,\n",
        "    do_sample: bool = False,\n",
        "    only_cpu: bool = False\n",
        "):\n",
        "  device = \"cpu\"\n",
        "\n",
        "  if not only_cpu:\n",
        "    if torch.cuda.is_available():\n",
        "      device = \"cuda\"\n",
        "    elif torch.backends.mps.is_available():\n",
        "      device = \"mps\"\n",
        "\n",
        "  print(\"Device in use: \", device)\n",
        "\n",
        "  print(f\"Loading model\")\n",
        "  model, tokenizer = load_hf_model(model_path, device)\n",
        "  model = model.to(device).eval()\n",
        "\n",
        "  num_image_tokens = model.config.vision_config.num_image_tokens\n",
        "  image_size = model.config.vision_config.image_size\n",
        "  processor = PaliGemmaProcessor(tokenizer, num_image_tokens, image_size)\n",
        "\n",
        "  print(\"Running inference\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    test_inference(\n",
        "        model,\n",
        "        processor,\n",
        "        device,\n",
        "        prompt,\n",
        "        image_file_path,\n",
        "        max_tokens_to_generate,\n",
        "        temperature,\n",
        "        top_p,\n",
        "        do_sample\n",
        "    )"
      ],
      "metadata": {
        "id": "-HAXnZzTPVvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmLLwexKPp_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}