{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl0CoFIijATU8rWVRtBK1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0nkly/pytorch_sandbox/blob/main/bondoGPT/bondoGPT_alpha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "# LOAD RAW DATA #\n",
        "#################"
      ],
      "metadata": {
        "id": "O4AhCRxyaQ4C"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# 1. Load the raw dataset (wikitext-2-raw-v1 is a good small choice)\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n",
        "\n",
        "cleaned_text = []\n",
        "for item in dataset:\n",
        "    text = item[\"text\"].strip()\n",
        "\n",
        "    # 2. Skip empty lines and section headers\n",
        "    if not text or text.startswith(\"=\"):\n",
        "        continue\n",
        "\n",
        "    # 3. Simple Markup Removal (you may need more sophisticated parsing)\n",
        "    # Remove internal links (e.g., [[text]]) and only keep the display text\n",
        "    text = re.sub(r'\\[\\[[^|\\]]+\\|([^\\]]+)\\]\\]', r'\\1', text)\n",
        "    text = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', text)\n",
        "\n",
        "    # Remove bold/italics markers (''')\n",
        "    text = text.replace(\"'''\", \"\").replace(\"''\", \"\")\n",
        "\n",
        "    # Filter out lines that might still contain complex templates or tags\n",
        "    if \"{{\" not in text and \"}}\" not in text:\n",
        "        cleaned_text.append(text)\n",
        "\n",
        "# 4. Join all the cleaned paragraphs into one large string\n",
        "final_corpus = \"\\n\\n\".join(cleaned_text)\n",
        "\n",
        "with open(\"tiny_wikitext.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWj6-IsmaWaZ",
        "outputId": "0ebca0e0-babf-4f66-c794-a8e03a7d469e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################\n",
        "# TRAIN TOKENIZER #\n",
        "###################"
      ],
      "metadata": {
        "id": "wCehvOFWa-e6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train a 2k-token tokenizer on your dataset\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    '--input=/home/bonda/tiny_wikitext.txt --model_prefix=tiny_sp2 --vocab_size=2048 --model_type=bpe'\n",
        ")\n",
        "\n",
        "# Load it\n",
        "import sentencepiece as spm\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"tiny_sp2.model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjGVthjjbDYk",
        "outputId": "28b40903-739c-4c2f-fb74-3bcf1d730c7a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/home/bonda/tiny_wikitext.txt --model_prefix=tiny_sp2 --vocab_size=2048 --model_type=bpe\n",
            "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
            "trainer_spec {\n",
            "  input: /home/bonda/tiny_wikitext.txt\n",
            "  input_format: \n",
            "  model_prefix: tiny_sp2\n",
            "  model_type: BPE\n",
            "  vocab_size: 2048\n",
            "  self_test_sample_size: 0\n",
            "  character_coverage: 0.9995\n",
            "  input_sentence_size: 0\n",
            "  shuffle_input_sentence: 1\n",
            "  seed_sentencepiece_size: 1000000\n",
            "  shrinking_factor: 0.75\n",
            "  max_sentence_length: 4192\n",
            "  num_threads: 16\n",
            "  num_sub_iterations: 2\n",
            "  max_sentencepiece_length: 16\n",
            "  split_by_unicode_script: 1\n",
            "  split_by_number: 1\n",
            "  split_by_whitespace: 1\n",
            "  split_digits: 0\n",
            "  pretokenization_delimiter: \n",
            "  treat_whitespace_as_suffix: 0\n",
            "  allow_whitespace_only_pieces: 0\n",
            "  required_chars: \n",
            "  byte_fallback: 0\n",
            "  vocabulary_output_piece_score: 1\n",
            "  train_extremely_large_corpus: 0\n",
            "  seed_sentencepieces_file: \n",
            "  hard_vocab_limit: 1\n",
            "  use_all_vocab: 0\n",
            "  unk_id: 0\n",
            "  bos_id: 1\n",
            "  eos_id: 2\n",
            "  pad_id: -1\n",
            "  unk_piece: <unk>\n",
            "  bos_piece: <s>\n",
            "  eos_piece: </s>\n",
            "  pad_piece: <pad>\n",
            "  unk_surface:  ⁇ \n",
            "  enable_differential_privacy: 0\n",
            "  differential_privacy_noise_level: 0\n",
            "  differential_privacy_clipping_threshold: 0\n",
            "}\n",
            "normalizer_spec {\n",
            "  name: nmt_nfkc\n",
            "  add_dummy_prefix: 1\n",
            "  remove_extra_whitespaces: 1\n",
            "  escape_whitespaces: 1\n",
            "  normalization_rule_tsv: \n",
            "}\n",
            "denormalizer_spec {}\n",
            "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
            "trainer_interface.cc(186) LOG(INFO) Loading corpus: /home/bonda/tiny_wikitext.txt\n",
            "trainer_interface.cc(411) LOG(INFO) Loaded all 17556 sentences\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n",
            "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n",
            "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
            "trainer_interface.cc(541) LOG(INFO) all chars count=10681287\n",
            "trainer_interface.cc(552) LOG(INFO) Done: 99.9503% characters are covered.\n",
            "trainer_interface.cc(562) LOG(INFO) Alphabet size=95\n",
            "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999503\n",
            "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 17556 sentences.\n",
            "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 17556\n",
            "trainer_interface.cc(611) LOG(INFO) Done! 75595\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=226304 min_freq=68\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64462 size=20 all=3502 active=1964 piece=en\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36907 size=40 all=4803 active=3265 piece=ion\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22631 size=60 all=6203 active=4665 piece=▁@\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17368 size=80 all=7766 active=6228 piece=ir\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14023 size=100 all=9220 active=7682 piece=ation\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13920 min_freq=1080\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11120 size=120 all=10655 active=2369 piece=os\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9493 size=140 all=12362 active=4076 piece=od\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7578 size=160 all=13870 active=5584 piece=pp\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6563 size=180 all=15715 active=7429 piece=ber\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5672 size=200 all=17043 active=8757 piece=ack\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5638 min_freq=938\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5004 size=220 all=18665 active=2534 piece=our\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4268 size=240 all=20009 active=3878 piece=cc\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3924 size=260 all=20875 active=4744 piece=▁whe\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3604 size=280 all=22022 active=5891 piece=ign\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3328 size=300 all=23000 active=6869 pie"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ce=▁has\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3321 min_freq=706\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3136 size=320 all=23673 active=1822 piece=▁sc\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2944 size=340 all=24705 active=2854 piece=▁part\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2701 size=360 all=25491 active=3640 piece=▁im\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2520 size=380 all=26394 active=4543 piece=▁pre\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2342 size=400 all=27450 active=5599 piece=▁would\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2341 min_freq=532\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2259 size=420 all=28157 active=2078 piece=ased\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2098 size=440 all=29284 active=3205 piece=ey\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1985 size=460 all=30253 active=4174 piece=▁most\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1904 size=480 all=30838 active=4759 piece=ug\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1796 size=500 all=31611 active=5532 piece=▁than\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1795 min_freq=432\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1712 size=520 all=32287 active=2256 piece=▁00\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1649 size=540 all=32733 active=2702 piece=▁pr\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1584 size=560 all=33332 active=3301 piece=olog\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1525 size=580 all=33761 active=3730 piece=▁second\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1456 size=600 all=34462 active=4431 piece=▁Jo\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1453 min_freq=369\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1388 size=620 all=34996 active=2240 piece=ars\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1331 size=640 all=35774 active=3018 piece=▁bl\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1272 size=660 all=36612 active=3856 piece=row\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1238 size=680 all=37022 active=4266 piece=▁add\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1207 size=700 all=37562 active=4806 piece=▁mon\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1206 min_freq=316\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1172 size=720 all=38211 active=2498 piece=ement\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1134 size=740 all=38725 active=3012 piece=ative\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1099 size=760 all=39246 active=3533 piece=ah\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1073 size=780 all=40143 active=4430 piece=▁197\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1046 size=800 all=40506 active=4793 piece=roup\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1046 min_freq=283\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1007 size=820 all=40784 active=2289 piece=gest\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=983 size=840 all=41448 active=2953 piece=▁resp\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=957 size=860 all=41796 active=3301 piece=▁prov\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=931 size=880 all=42031 active=3536 piece=▁use\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=907 size=900 all=42426 active=3931 piece=ole\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=907 min_freq=255\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=882 size=920 all=42983 active=2616 piece=▁Sept\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=864 size=940 all=43307 active=2940 piece=▁character\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=835 size=960 all=43687 active=3320 piece=▁ever\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=816 size=980 all=44155 active=3788 piece=▁191\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=797 size=1000 all=44799 active=4432 piece=ives\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=796 min_freq=232\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=777 size=1020 all=45107 active=2512 piece=▁Gu\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=754 size=1040 all=45395 active=2800 piece=ility\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=744 size=1060 all=45823 active=3228 piece=ified\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=732 size=1080 all=46214 active=3619 piece=▁wrote\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=713 size=1100 all=46735 active=4140 piece=▁short\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=712 min_freq=213\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=700 size=1120 all=47142 active=2733 piece=▁camp\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=688 size=1140 all=47501 active=3092 piece=▁30\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=675 size=1160 all=47763 active=3354 piece=▁care\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=662 size="
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################\n",
        "# CREATE DATASET #\n",
        "##################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvqM3MDMcHlP",
        "outputId": "c68afa40-3903-478a-819f-416c868fafd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "eq=477 size=1520 all=53240 active=2910 piece=▁nov\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=468 size=1540 all=53481 active=3151 piece=▁Old\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=460 size=1560 all=53804 active=3474 piece=▁sk\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=451 size=1580 all=54225 active=3895 piece=omen\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=446 size=1600 all=54500 active=4170 piece=▁establ\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=446 min_freq=155\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=442 size=1620 all=54755 active=2979 piece=▁plan\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=433 size=1640 all=55059 active=3283 piece=▁announ\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=427 size=1660 all=55307 active=3531 piece=▁27\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=423 size=1680 all=55655 active=3879 piece=igade\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=417 size=1700 all=55923 active=4147 piece=▁good\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=417 min_freq=146\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=409 size=1720 all=56254 active=3123 piece=▁wat\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=402 size=1740 all=56538 active=3407 piece=▁bo\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=398 size=1760 all=56752 active=3621 piece=▁ten\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=393 size=1780 all=57016 active=3885 piece=▁red\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=387 size=1800 all=57211 active=4080 piece=iting\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=387 min_freq=139\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=380 size=1820 all=57520 active=3133 piece=aign\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=374 size=1840 all=57866 active=3479 piece=ument\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=370 size=1860 all=58397 active=4010 piece=alth\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=364 size=1880 all=58655 active=4268 piece=idd\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=358 size=1900 all=58907 active=4520 piece=▁fac\n",
            "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=358 min_freq=131\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=354 size=1920 all=59028 active=3053 piece=▁rank\n",
            "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=350 size=1940 all=59196 active=3221 piece=▁little\n",
            "trainer_interface.cc(689) LOG(INFO) Saving model: tiny_sp2.model\n",
            "trainer_interface.cc(701) LOG(INFO) Saving vocabs: tiny_sp2.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import sentencepiece as spm\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, sp_model_path, text_path, block_size=128):\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "        self.sp.load(sp_model_path)\n",
        "        with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        ids = self.sp.encode(text, out_type=int)\n",
        "        self.data = torch.tensor(ids, dtype=torch.long)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(1, (len(self.data) - 1) // self.block_size)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.block_size\n",
        "        x = self.data[start:start+self.block_size]\n",
        "        y = self.data[start+1:start+1+self.block_size]\n",
        "        # pad if necessary\n",
        "        if x.size(0) < self.block_size:\n",
        "            pad = torch.full((self.block_size - x.size(0),), 0, dtype=torch.long)\n",
        "            x = torch.cat([x, pad])\n",
        "            y = torch.cat([y, pad])\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "5xT_iirgcMm6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "# CREATE MODEL #\n",
        "################"
      ],
      "metadata": {
        "id": "IHoTby18b4bF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size=2048, block_size=128, n_layer=6, n_head=8, n_embd=256):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.n_layer = n_layer\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        assert cfg.n_embd % cfg.n_head == 0\n",
        "        self.n_head = cfg.n_head\n",
        "        self.head_dim = cfg.n_embd // cfg.n_head\n",
        "        self.c_attn = nn.Linear(cfg.n_embd, 3 * cfg.n_embd)\n",
        "        self.c_proj = nn.Linear(cfg.n_embd, cfg.n_embd)\n",
        "        # causal mask will be applied using broadcast\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(cfg.block_size, cfg.block_size)).unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)  # (B,T,3C)\n",
        "        q, k, v = qkv.split(C, dim=2)\n",
        "        # reshape heads\n",
        "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1,2)  # (B, nh, T, hd)\n",
        "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
        "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1,2)\n",
        "        att = (q @ k.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
        "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1,2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.attn = CausalSelfAttention(cfg)\n",
        "        self.ln2 = nn.LayerNorm(cfg.n_embd)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.n_embd, 4*cfg.n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4*cfg.n_embd, cfg.n_embd)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, cfg: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.n_embd)\n",
        "        self.pos_emb = nn.Embedding(cfg.block_size, cfg.n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(cfg) for _ in range(cfg.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(cfg.n_embd)\n",
        "        self.head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.cfg.block_size\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=50, temperature=1.0):\n",
        "        self.eval()\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.cfg.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            probs = F.softmax(logits[:, -1, :]/temperature, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_id), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "H_pZ1b-K8AVh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "# TRAIN MODEL #\n",
        "###############"
      ],
      "metadata": {
        "id": "402K5_FPc4fk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import trange, tqdm\n",
        "\n",
        "sp_model = \"/home/bonda/tiny_sp.model\"\n",
        "data = \"/home/bonda/tiny_wikitext.txt\"\n",
        "out_dir = \"/home/bonda/checkpoints\"\n",
        "batch_size = 32\n",
        "block_size = 768\n",
        "lr = 2e-4\n",
        "epochs = 10\n",
        "device = \"cuda\"\n",
        "\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "cfg = GPTConfig(vocab_size=2048, block_size=block_size, n_layer=6, n_head=8, n_embd=256)\n",
        "model = TinyGPT(cfg).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "dataset = CharDataset(sp_model, data, block_size=block_size)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=2)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
        "    for x, y in pbar:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            logits, loss = model(x, targets=y)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        global_step += 1\n",
        "        if global_step % 100 == 0:\n",
        "            # save checkpoint\n",
        "            ckpt = os.path.join(out_dir, f\"ckpt_step{global_step}.pt\")\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optim_state\": optimizer.state_dict(),\n",
        "                \"step\": global_step,\n",
        "                \"cfg\": vars(cfg)\n",
        "            }, ckpt)\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "# final save\n",
        "torch.save({\"model_state\": model.state_dict(), \"cfg\": vars(cfg)}, os.path.join(out_dir, \"final.pt\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th9Qe9sN9crc",
        "outputId": "c15882aa-350b-4603-c6dc-1bbcbe39dc04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_33533/2897286319.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Epoch 1:   0%|                                                                                                | 0/177 [00:00<?, ?it/s]/tmp/ipykernel_33533/2897286319.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [00:59<00:00,  2.97it/s, loss=4.6850]\n",
            "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:00<00:00,  2.91it/s, loss=4.2701]\n",
            "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:01<00:00,  2.86it/s, loss=4.1697]\n",
            "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:02<00:00,  2.84it/s, loss=4.0691]\n",
            "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:02<00:00,  2.84it/s, loss=3.9804]\n",
            "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:02<00:00,  2.82it/s, loss=3.9375]\n",
            "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:03<00:00,  2.81it/s, loss=3.9080]\n",
            "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:04<00:00,  2.75it/s, loss=3.8880]\n",
            "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████| 177/177 [01:03<00:00,  2.81it/s, loss=3.8506]\n",
            "Epoch 10: 100%|████████████████████████████████████████████████████████████████████████| 177/177 [01:03<00:00,  2.79it/s, loss=3.7595]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############\n",
        "# INFERENCE #\n",
        "#############"
      ],
      "metadata": {
        "id": "xAukSl7oc-ol"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sentencepiece as spm\n",
        "\n",
        "ckpt = \"checkpoints/final.pt\"\n",
        "prompt = \"A car is something that\"\n",
        "max_new = 1000\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(sp_model)\n",
        "\n",
        "ck = torch.load(ckpt, map_location=\"cpu\")\n",
        "cfg = GPTConfig(**ck[\"cfg\"]) if \"cfg\" in ck else GPTConfig()\n",
        "model = TinyGPT(cfg)\n",
        "model.load_state_dict(ck[\"model_state\"])\n",
        "model = model.cuda().eval()\n",
        "\n",
        "ids = sp.encode(prompt, out_type=int)\n",
        "import torch\n",
        "x = torch.tensor([ids], dtype=torch.long).cuda()\n",
        "out = model.generate(x, max_new_tokens=max_new)\n",
        "print(sp.decode(out[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFJufbiV_vQC",
        "outputId": "411aa077-fff4-4812-ead6-544faf9e3368"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_33533/1475036476.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ck = torch.load(ckpt, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A car is something that be on iachter simaksideddened with that with the Union at Stale conse zze and foundations max Blubeaneciccal of the Aveacimits weudy . Dowsonster propative , Mz endsca  ⁇  deep then Unityrica  ⁇   ⁇  is one of the music Res  ⁇  that the lated or in the black wanted with the also tarts of a  ⁇  it is perportray  ⁇ - ⁇  side of the books are tacagaz to Se  ⁇  whisticult er twenty . valuesmenies of the Video .RS has does not upon  ⁇  speak Estembergles  ⁇ - ⁇  to  ⁇ - ⁇  prown raidental unlovelos was succ swucklearet and epicactorp that the diredlish Par in the first ,  ⁇  form ganaissues and about battle star  ⁇  so  ⁇ - ⁇  bassinated A. Charles called , Sir press the In about . The gave the Uylauring the best needed as the most truck is damentatory that Mey  ⁇  Ma  ⁇ ly  ⁇  charababy region to decricrial Na Ber . King Greatyan paintedib, others have wart . Stone  ⁇  greena 's arey place that it does noted in starel of the locrigs  ⁇  each followed to but Darcript to Baring the old Res als every crush in Port trad Prentun amazine of Cia  ⁇  shareheonestaby that was the vice incoral Septasyround . Thely yet a nures , by the most outso BN Cam slow site Stademm. Instrud to make it 'sermanitish  ⁇ d the Undering was origion of this dozerlairemented Line  ⁇  The Canad  ⁇ - ⁇   ⁇  book and obs the reseen from concters within older d ⁇  ignater player , Nation of parally visiting stalkers have been proper Americocad actression Raled Sing and Northougher flying the Mndernerategius earlipping majories as  ⁇  , with histories wention career commempany , Ch.Cistority  ⁇  , she has descons uritagh , pressoes that the moved Eviewinderred in . Pritute Fied by Don ' not are open Home Ravies  ⁇   ⁇  his chiefiticine her daughter emergesonencters  ⁇  murdered in the creat Campace to soul working case , Prac  ⁇   ⁇  , the CO wanted that he canution to one of the Play Chibers and blacks her guest , and praise themeKentreck the or to her sistertern the be writic pos or prim  ⁇  pointing simate the New York Chanen 's arricts cerved war defen ' . Odreached accorded State Warren standard Dayed the city is a the necessity of the B. Thatly , both valings and was namating for Lis to the , the ep  ⁇  obtains with cre canycisors on his eas were because of these popment proding of Dered the patration . the Sbined of children  ⁇  has seriously to get this land . A laniphys areacation of the cellar  ⁇  and reviration compl idered to have been labilar prime Anders whose signing as drawn  ⁇ - ⁇  . . She followed to have performernorth Peter lan Secreting Engl a signiffering that Israd the de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F6oljlIid165"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}